

  0%|▎                                                                                                                                                                         | 1/630 [00:04<51:57,  4.96s/it]


































































































 16%|██████████████████████████▋                                                                                                                                             | 100/630 [04:00<20:45,  2.35s/it]



































































































 32%|█████████████████████████████████████████████████████▎                                                                                                                  | 200/630 [07:51<15:59,  2.23s/it][INFO|trainer.py:3081] 2023-10-22 20:13:10,182 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-22 20:13:10,182 >>   Num examples = 4
[INFO|trainer.py:3086] 2023-10-22 20:13:10,182 >>   Batch size = 1
{'loss': 0.9383, 'learning_rate': 8.007135645447982e-06, 'epoch': 9.2}
{'eval_loss': 0.768081784248352, 'eval_runtime': 1.0556, 'eval_samples_per_second': 3.789, 'eval_steps_per_second': 1.895, 'epoch': 9.2}
10/22/2023 20:13:11 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-200
10/22/2023 20:13:11 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
 32%|█████████████████████████████████████████████████████▎                                                                                                                  | 200/630 [07:52<15:59,  2.23s/it][INFO|tokenization_utils_base.py:2210] 2023-10-22 20:13:11,480 >> tokenizer config file saved in sft_model_path/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-10-22 20:13:11,480 >> Special tokens file saved in sft_model_path/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-22 20:13:11,935 >> Deleting older checkpoint [sft_model_path/checkpoint-400] due to args.save_total_limit


































































































 47%|███████████████████████████████████████████████████████████████████████████████▋                                                                                        | 299/630 [11:38<13:02,  2.36s/it]




































































































 63%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                             | 399/630 [15:21<07:56,  2.06s/it]
 63%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                             | 400/630 [15:23<07:53,  2.06s/it][INFO|trainer.py:3081] 2023-10-22 20:20:42,326 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-22 20:20:42,327 >>   Num examples = 4
[INFO|trainer.py:3086] 2023-10-22 20:20:42,327 >>   Batch size = 1
  0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]
{'eval_loss': 0.6752180457115173, 'eval_runtime': 1.0643, 'eval_samples_per_second': 3.758, 'eval_steps_per_second': 1.879, 'epoch': 18.39}
10/22/2023 20:20:43 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-400
[INFO|tokenization_utils_base.py:2217] 2023-10-22 20:20:43,638 >> Special tokens file saved in sft_model_path/checkpoint-400/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-22 20:20:44,103 >> Deleting older checkpoint [sft_model_path/checkpoint-200] due to args.save_total_limit


































































































 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                   | 499/630 [18:50<04:22,  2.00s/it]



































































































 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋        | 599/630 [22:23<01:02,  2.02s/it]
 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████        | 600/630 [22:25<01:00,  2.02s/it][INFO|trainer.py:3081] 2023-10-22 20:27:44,273 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-22 20:27:44,273 >>   Num examples = 4
[INFO|trainer.py:3086] 2023-10-22 20:27:44,273 >>   Batch size = 1
 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████        | 600/630 [22:26<01:00,  2.02s/it][INFO|tokenization_utils_base.py:2210] 2023-10-22 20:27:45,634 >> tokenizer config file saved in sft_model_path/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-10-22 20:27:45,635 >> Special tokens file saved in sft_model_path/checkpoint-600/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-22 20:27:46,085 >> Deleting older checkpoint [sft_model_path/checkpoint-400] due to args.save_total_limit
{'eval_loss': 0.6279358267784119, 'eval_runtime': 1.116, 'eval_samples_per_second': 3.584, 'eval_steps_per_second': 1.792, 'epoch': 27.59}
10/22/2023 20:27:45 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-600
10/22/2023 20:27:45 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



























100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 630/630 [23:29<00:00,  1.99s/it][INFO|trainer.py:1934] 2023-10-22 20:28:48,161 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 630/630 [23:29<00:00,  2.24s/it]
{'train_runtime': 1419.0358, 'train_samples_per_second': 7.336, 'train_steps_per_second': 0.444, 'train_loss': 0.8904537170652359, 'epoch': 28.97}
10/22/2023 20:28:48 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
10/22/2023 20:28:48 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =      28.97
  train_loss               =     0.8905
  train_runtime            = 0:23:39.03
  train_samples_per_second =      7.336
  train_steps_per_second   =      0.444
[INFO|tokenization_utils_base.py:2210] 2023-10-22 20:28:48,555 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-10-22 20:28:48,555 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-10-22 20:28:48,564 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-22 20:28:48,564 >>   Num examples = 4
[INFO|trainer.py:3086] 2023-10-22 20:28:48,564 >>   Batch size = 1
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10.03it/s]
***** eval metrics *****
  epoch                   =      28.97
  eval_loss               =      0.628
  eval_runtime            = 0:00:01.01
  eval_samples_per_second =       3.96
  eval_steps_per_second   =       1.98
  perplexity              =     1.8739