

  0%|                                                                                                                                                                 | 1/1885 [00:04<2:19:05,  4.43s/it]


































































































  5%|████████▍                                                                                                                                                       | 99/1885 [03:47<1:08:48,  2.31s/it]




































































































 11%|████████████████▊                                                                                                                                              | 199/1885 [07:35<1:04:55,  2.31s/it]





































































































 16%|█████████████████████████▎                                                                                                                                     | 300/1885 [11:36<1:03:53,  2.42s/it]



































































































 21%|██████████████████████████████████                                                                                                                               | 399/1885 [15:28<56:39,  2.29s/it]




































































































 26%|██████████████████████████████████████████▌                                                                                                                      | 499/1885 [19:27<54:21,  2.35s/it]
 27%|██████████████████████████████████████████▋                                                                                                                      | 500/1885 [19:29<53:20,  2.31s/it][INFO|trainer.py:3081] 2023-12-06 19:34:12,254 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 19:34:12,255 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 19:34:12,255 >>   Batch size = 1

 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋     | 30/31 [00:02<00:00, 11.87it/s]
{'eval_loss': 0.49144247174263, 'eval_runtime': 3.8119, 'eval_samples_per_second': 16.002, 'eval_steps_per_second': 8.132, 'epoch': 1.32}
12/06/2023 19:34:16 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-500

[INFO|tokenization_utils_base.py:2217] 2023-12-06 19:34:16,349 >> Special tokens file saved in sft_model_path/checkpoint-500/special_tokens_map.json



































































































 32%|███████████████████████████████████████████████████▏                                                                                                             | 600/1885 [23:29<50:34,  2.36s/it]




































































































 37%|███████████████████████████████████████████████████████████▊                                                                                                     | 700/1885 [27:39<58:50,  2.98s/it]



































































































 42%|████████████████████████████████████████████████████████████████████▏                                                                                            | 799/1885 [32:40<52:14,  2.89s/it]




































































































 48%|████████████████████████████████████████████████████████████████████████████▊                                                                                    | 899/1885 [37:37<48:38,  2.96s/it]




































































































 53%|█████████████████████████████████████████████████████████████████████████████████████▎                                                                           | 999/1885 [42:26<43:29,  2.95s/it]
 53%|████████████████████████████████████████████████████████████████████████████████████▉                                                                           | 1000/1885 [42:30<48:12,  3.27s/it][INFO|trainer.py:3081] 2023-12-06 19:57:13,868 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 19:57:13,868 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 19:57:13,868 >>   Batch size = 1

 48%|███████████████████████████████████████████████████████████████████████████████▎                                                                                    | 15/31 [00:01<00:01,  9.27it/s]
{'eval_loss': 0.43262436985969543, 'eval_runtime': 5.065, 'eval_samples_per_second': 12.043, 'eval_steps_per_second': 6.12, 'epoch': 2.65}
12/06/2023 19:57:18 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1000
 53%|████████████████████████████████████████████████████████████████████████████████████▉                                                                           | 1000/1885 [42:35<48:12,  3.27s/it][INFO|tokenization_utils_base.py:2210] 2023-12-06 19:57:19,217 >> tokenizer config file saved in sft_model_path/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-06 19:57:19,218 >> Special tokens file saved in sft_model_path/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-06 19:57:19,761 >> Deleting older checkpoint [sft_model_path/checkpoint-500] due to args.save_total_limit


































































































 58%|█████████████████████████████████████████████████████████████████████████████████████████████▎                                                                  | 1099/1885 [47:32<40:59,  3.13s/it]





































































































 64%|█████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                          | 1200/1885 [52:28<33:25,  2.93s/it]




































































































 69%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                 | 1300/1885 [57:16<26:11,  2.69s/it]



































































































 74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 1399/1885 [1:02:03<24:24,  3.01s/it]




































































































 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                | 1499/1885 [1:06:50<19:28,  3.03s/it]
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                | 1500/1885 [1:06:53<19:38,  3.06s/it][INFO|trainer.py:3081] 2023-12-06 20:21:36,532 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 20:21:36,532 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 20:21:36,533 >>   Batch size = 1

 71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                               | 22/31 [00:02<00:00,  9.31it/s]
{'eval_loss': 0.4152982532978058, 'eval_runtime': 5.5879, 'eval_samples_per_second': 10.916, 'eval_steps_per_second': 5.548, 'epoch': 3.97}
12/06/2023 20:21:42 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1500

 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                | 1500/1885 [1:06:59<19:38,  3.06s/it][INFO|tokenization_utils_base.py:2210] 2023-12-06 20:21:42,518 >> tokenizer config file saved in sft_model_path/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-06 20:21:42,520 >> Special tokens file saved in sft_model_path/checkpoint-1500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-06 20:21:43,195 >> Deleting older checkpoint [sft_model_path/checkpoint-1000] due to args.save_total_limit


































































































 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                        | 1599/1885 [1:11:50<14:50,  3.11s/it]




































































































 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍               | 1699/1885 [1:16:32<09:26,  3.05s/it]





































































































 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉       | 1800/1885 [1:21:30<04:00,  2.83s/it]




















































































100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1885/1885 [1:25:37<00:00,  2.90s/it][INFO|trainer.py:1934] 2023-12-06 20:40:20,411 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1885/1885 [1:25:37<00:00,  2.73s/it]
{'train_runtime': 5147.4679, 'train_samples_per_second': 5.866, 'train_steps_per_second': 0.366, 'train_loss': 0.6586447819474521, 'epoch': 4.99}
12/06/2023 20:40:20 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
12/06/2023 20:40:20 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =       4.99
  train_loss               =     0.6586
  train_runtime            = 1:25:47.46
  train_samples_per_second =      5.866
  train_steps_per_second   =      0.366
[INFO|tokenization_utils_base.py:2210] 2023-12-06 20:40:21,203 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-06 20:40:21,203 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-12-06 20:40:21,213 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 20:40:21,214 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 20:40:21,214 >>   Batch size = 1


100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 10.54it/s]
***** eval metrics *****
  epoch                   =       4.99
  eval_loss               =     0.4146
  eval_runtime            = 0:00:04.51
  eval_samples_per_second =     13.508
  eval_steps_per_second   =      6.865
  perplexity              =     1.5137