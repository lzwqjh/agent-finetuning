

  0%|                                                                                                                                                    | 1/3155 [00:04<3:58:10,  4.53s/it]


































































































  3%|████▌                                                                                                                                              | 99/3155 [03:45<1:46:14,  2.09s/it]



































































































  6%|█████████▎                                                                                                                                        | 200/3155 [07:17<1:43:37,  2.10s/it][INFO|trainer.py:3081] 2023-10-24 11:51:01,872 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-24 11:51:01,872 >>   Num examples = 102
[INFO|trainer.py:3086] 2023-10-24 11:51:01,872 >>   Batch size = 1
{'loss': 1.2975, 'learning_rate': 9.970976158864074e-06, 'epoch': 0.32}

 76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                   | 39/51 [00:03<00:00, 12.54it/s]
{'eval_loss': 1.2263062000274658, 'eval_runtime': 5.1743, 'eval_samples_per_second': 19.713, 'eval_steps_per_second': 9.856, 'epoch': 0.32}
10/24/2023 11:51:07 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-200
[INFO|tokenization_utils_base.py:2217] 2023-10-24 11:51:07,287 >> Special tokens file saved in sft_model_path/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-24 11:51:07,730 >> Deleting older checkpoint [sft_model_path/checkpoint-600] due to args.save_total_limit


































































































  9%|█████████████▊                                                                                                                                    | 299/3155 [10:46<1:36:05,  2.02s/it]




































































































 13%|██████████████████▌                                                                                                                               | 400/3155 [14:13<1:34:02,  2.05s/it][INFO|trainer.py:3081] 2023-10-24 11:57:58,525 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-24 11:57:58,525 >>   Num examples = 102
[INFO|trainer.py:3086] 2023-10-24 11:57:58,525 >>   Batch size = 1
{'loss': 1.1919, 'learning_rate': 9.756866358597819e-06, 'epoch': 0.63}

 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                 | 45/51 [00:03<00:00, 12.73it/s]
{'eval_loss': 1.1834396123886108, 'eval_runtime': 4.8647, 'eval_samples_per_second': 20.968, 'eval_steps_per_second': 10.484, 'epoch': 0.63}
10/24/2023 11:58:03 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-400
[INFO|tokenization_utils_base.py:2217] 2023-10-24 11:58:03,619 >> Special tokens file saved in sft_model_path/checkpoint-400/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-24 11:58:04,047 >> Deleting older checkpoint [sft_model_path/checkpoint-200] due to args.save_total_limit


































































































 16%|███████████████████████                                                                                                                           | 499/3155 [17:39<1:29:16,  2.02s/it]



































































































 19%|███████████████████████████▊                                                                                                                      | 600/3155 [21:00<1:25:22,  2.00s/it][INFO|trainer.py:3081] 2023-10-24 12:04:45,619 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-24 12:04:45,619 >>   Num examples = 102
[INFO|trainer.py:3086] 2023-10-24 12:04:45,619 >>   Batch size = 1
{'loss': 1.1422, 'learning_rate': 9.342903214447056e-06, 'epoch': 0.95}

[INFO|tokenization_utils_base.py:2217] 2023-10-24 12:04:50,677 >> Special tokens file saved in sft_model_path/checkpoint-600/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-24 12:04:51,109 >> Deleting older checkpoint [sft_model_path/checkpoint-400] due to args.save_total_limit
{'eval_loss': 1.1382970809936523, 'eval_runtime': 4.826, 'eval_samples_per_second': 21.136, 'eval_steps_per_second': 10.568, 'epoch': 0.95}
10/24/2023 12:04:50 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-600
10/24/2023 12:04:50 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.

































































































 22%|████████████████████████████████▍                                                                                                                 | 700/3155 [24:30<1:25:57,  2.10s/it]


































































































 25%|████████████████████████████████████▉                                                                                                             | 799/3155 [27:54<1:21:36,  2.08s/it]
 25%|█████████████████████████████████████                                                                                                             | 800/3155 [27:56<1:21:38,  2.08s/it][INFO|trainer.py:3081] 2023-10-24 12:11:41,107 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-24 12:11:41,107 >>   Num examples = 102
[INFO|trainer.py:3086] 2023-10-24 12:11:41,107 >>   Batch size = 1


[INFO|tokenization_utils_base.py:2217] 2023-10-24 12:11:46,204 >> Special tokens file saved in sft_model_path/checkpoint-800/special_tokens_map.json
{'eval_loss': 1.126626968383789, 'eval_runtime': 4.8438, 'eval_samples_per_second': 21.058, 'eval_steps_per_second': 10.529, 'epoch': 1.27}
10/24/2023 12:11:45 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-800
10/24/2023 12:11:45 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2894] 2023-10-24 12:11:46,653 >> Deleting older checkpoint [sft_model_path/checkpoint-600] due to args.save_total_limit
































































































 28%|█████████████████████████████████████████▌                                                                                                        | 899/3155 [31:22<1:15:28,  2.01s/it]



































































































 32%|█████████████████████████████████████████████▉                                                                                                   | 1000/3155 [34:45<1:12:17,  2.01s/it][INFO|trainer.py:3081] 2023-10-24 12:18:29,868 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-24 12:18:29,868 >>   Num examples = 102
[INFO|trainer.py:3086] 2023-10-24 12:18:29,868 >>   Batch size = 1
{'loss': 1.1239, 'learning_rate': 7.992651177623466e-06, 'epoch': 1.58}

 84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                       | 43/51 [00:03<00:00, 12.75it/s]
{'eval_loss': 1.1187461614608765, 'eval_runtime': 4.8548, 'eval_samples_per_second': 21.01, 'eval_steps_per_second': 10.505, 'epoch': 1.58}
10/24/2023 12:18:34 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1000
[INFO|tokenization_utils_base.py:2217] 2023-10-24 12:18:34,956 >> Special tokens file saved in sft_model_path/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-24 12:18:35,383 >> Deleting older checkpoint [sft_model_path/checkpoint-800] due to args.save_total_limit

































































































 35%|██████████████████████████████████████████████████▌                                                                                              | 1099/3155 [38:09<1:09:00,  2.01s/it]



































































































 38%|███████████████████████████████████████████████████████▏                                                                                         | 1200/3155 [41:31<1:05:19,  2.00s/it][INFO|trainer.py:3081] 2023-10-24 12:25:16,442 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-24 12:25:16,442 >>   Num examples = 102
[INFO|trainer.py:3086] 2023-10-24 12:25:16,442 >>   Batch size = 1
{'loss': 1.1065, 'learning_rate': 7.113091308703498e-06, 'epoch': 1.9}

 96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████      | 49/51 [00:03<00:00, 12.91it/s]
{'eval_loss': 1.1122369766235352, 'eval_runtime': 4.8077, 'eval_samples_per_second': 21.216, 'eval_steps_per_second': 10.608, 'epoch': 1.9}
10/24/2023 12:25:21 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1200
[INFO|tokenization_utils_base.py:2217] 2023-10-24 12:25:21,481 >> Special tokens file saved in sft_model_path/checkpoint-1200/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-24 12:25:21,916 >> Deleting older checkpoint [sft_model_path/checkpoint-1000] due to args.save_total_limit



































































































 41%|███████████████████████████████████████████████████████████▋                                                                                     | 1300/3155 [45:04<1:01:39,  1.99s/it]



































































































 44%|████████████████████████████████████████████████████████████████▎                                                                                | 1400/3155 [48:32<1:01:29,  2.10s/it][INFO|trainer.py:3081] 2023-10-24 12:32:16,958 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-24 12:32:16,958 >>   Num examples = 102
[INFO|trainer.py:3086] 2023-10-24 12:32:16,958 >>   Batch size = 1
  0%|                                                                                                                                                                | 0/51 [00:00<?, ?it/s]


 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 50/51 [00:04<00:00, 12.27it/s]
{'eval_loss': 1.107033371925354, 'eval_runtime': 5.1348, 'eval_samples_per_second': 19.864, 'eval_steps_per_second': 9.932, 'epoch': 2.22}
10/24/2023 12:32:22 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1400
[INFO|tokenization_utils_base.py:2217] 2023-10-24 12:32:22,347 >> Special tokens file saved in sft_model_path/checkpoint-1400/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-24 12:32:22,814 >> Deleting older checkpoint [sft_model_path/checkpoint-1200] due to args.save_total_limit

































































































 48%|█████████████████████████████████████████████████████████████████████▊                                                                             | 1499/3155 [52:03<56:26,  2.04s/it]

































































































 51%|██████████████████████████████████████████████████████████████████████████▌                                                                        | 1599/3155 [55:28<54:08,  2.09s/it]
 51%|██████████████████████████████████████████████████████████████████████████▌                                                                        | 1600/3155 [55:30<54:16,  2.09s/it][INFO|trainer.py:3081] 2023-10-24 12:39:15,164 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-24 12:39:15,164 >>   Num examples = 102
[INFO|trainer.py:3086] 2023-10-24 12:39:15,164 >>   Batch size = 1


[INFO|tokenization_utils_base.py:2217] 2023-10-24 12:39:20,809 >> Special tokens file saved in sft_model_path/checkpoint-1600/special_tokens_map.json
{'eval_loss': 1.103785753250122, 'eval_runtime': 5.3535, 'eval_samples_per_second': 19.053, 'eval_steps_per_second': 9.526, 'epoch': 2.54}
10/24/2023 12:39:20 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1600
10/24/2023 12:39:20 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2894] 2023-10-24 12:39:21,360 >> Deleting older checkpoint [sft_model_path/checkpoint-1400] due to args.save_total_limit



































































































 54%|███████████████████████████████████████████████████████████████████████████████▏                                                                   | 1700/3155 [59:07<50:58,  2.10s/it]



































































































 57%|██████████████████████████████████████████████████████████████████████████████████▋                                                              | 1799/3155 [1:02:45<45:30,  2.01s/it]
 57%|██████████████████████████████████████████████████████████████████████████████████▋                                                              | 1800/3155 [1:02:48<45:48,  2.03s/it][INFO|trainer.py:3081] 2023-10-24 12:46:32,721 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-24 12:46:32,721 >>   Num examples = 102
[INFO|trainer.py:3086] 2023-10-24 12:46:32,721 >>   Batch size = 1


100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:03<00:00, 10.75it/s]
{'eval_loss': 1.10077965259552, 'eval_runtime': 4.7515, 'eval_samples_per_second': 21.467, 'eval_steps_per_second': 10.733, 'epoch': 2.85}
10/24/2023 12:46:37 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1800
[INFO|tokenization_utils_base.py:2217] 2023-10-24 12:46:37,731 >> Special tokens file saved in sft_model_path/checkpoint-1800/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-24 12:46:38,177 >> Deleting older checkpoint [sft_model_path/checkpoint-1600] due to args.save_total_limit

































































































 60%|███████████████████████████████████████████████████████████████████████████████████████▎                                                         | 1899/3155 [1:06:16<44:22,  2.12s/it]

































































































 63%|███████████████████████████████████████████████████████████████████████████████████████████▊                                                     | 1999/3155 [1:09:46<35:42,  1.85s/it]
 63%|███████████████████████████████████████████████████████████████████████████████████████████▉                                                     | 2000/3155 [1:09:48<35:22,  1.84s/it][INFO|trainer.py:3081] 2023-10-24 12:53:33,147 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-24 12:53:33,148 >>   Num examples = 102
[INFO|trainer.py:3086] 2023-10-24 12:53:33,148 >>   Batch size = 1


[INFO|tokenization_utils_base.py:2217] 2023-10-24 12:53:38,184 >> Special tokens file saved in sft_model_path/checkpoint-2000/special_tokens_map.json
{'eval_loss': 1.0984386205673218, 'eval_runtime': 4.7962, 'eval_samples_per_second': 21.267, 'eval_steps_per_second': 10.633, 'epoch': 3.17}
10/24/2023 12:53:37 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2000
10/24/2023 12:53:37 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2894] 2023-10-24 12:53:38,638 >> Deleting older checkpoint [sft_model_path/checkpoint-1800] due to args.save_total_limit




























































































 67%|████████████████████████████████████████████████████████████████████████████████████████████████▍                                                | 2099/3155 [1:13:07<36:22,  2.07s/it]




































































































 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████                                            | 2200/3155 [1:16:32<32:32,  2.04s/it][INFO|trainer.py:3081] 2023-10-24 13:00:17,545 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-24 13:00:17,545 >>   Num examples = 102
[INFO|trainer.py:3086] 2023-10-24 13:00:17,545 >>   Batch size = 1
{'loss': 1.0805, 'learning_rate': 2.2168158729692484e-06, 'epoch': 3.49}

 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                 | 45/51 [00:03<00:00, 12.97it/s]
{'eval_loss': 1.096867322921753, 'eval_runtime': 4.7856, 'eval_samples_per_second': 21.314, 'eval_steps_per_second': 10.657, 'epoch': 3.49}
10/24/2023 13:00:22 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2200
[INFO|tokenization_utils_base.py:2217] 2023-10-24 13:00:22,567 >> Special tokens file saved in sft_model_path/checkpoint-2200/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-24 13:00:23,009 >> Deleting older checkpoint [sft_model_path/checkpoint-2000] due to args.save_total_limit


























































































 73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                       | 2300/3155 [1:19:41<26:05,  1.83s/it]


























































































 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                  | 2400/3155 [1:22:43<22:53,  1.82s/it][INFO|trainer.py:3081] 2023-10-24 13:06:28,144 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-24 13:06:28,144 >>   Num examples = 102
[INFO|trainer.py:3086] 2023-10-24 13:06:28,144 >>   Batch size = 1
{'loss': 1.0751, 'learning_rate': 1.4283549964890298e-06, 'epoch': 3.8}

 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████         | 48/51 [00:03<00:00, 13.20it/s]
{'eval_loss': 1.0960137844085693, 'eval_runtime': 4.7715, 'eval_samples_per_second': 21.377, 'eval_steps_per_second': 10.689, 'epoch': 3.8}
10/24/2023 13:06:32 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2400
[INFO|tokenization_utils_base.py:2217] 2023-10-24 13:06:33,147 >> Special tokens file saved in sft_model_path/checkpoint-2400/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-24 13:06:33,585 >> Deleting older checkpoint [sft_model_path/checkpoint-2200] due to args.save_total_limit


























































































 79%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                              | 2500/3155 [1:25:51<20:03,  1.84s/it]


























































































 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                         | 2600/3155 [1:28:56<16:53,  1.83s/it][INFO|trainer.py:3081] 2023-10-24 13:12:40,764 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-24 13:12:40,764 >>   Num examples = 102
[INFO|trainer.py:3086] 2023-10-24 13:12:40,765 >>   Batch size = 1
{'loss': 1.0709, 'learning_rate': 7.899519823302743e-07, 'epoch': 4.12}

[INFO|tokenization_utils_base.py:2217] 2023-10-24 13:12:45,773 >> Special tokens file saved in sft_model_path/checkpoint-2600/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-24 13:12:46,207 >> Deleting older checkpoint [sft_model_path/checkpoint-2400] due to args.save_total_limit
{'eval_loss': 1.0953569412231445, 'eval_runtime': 4.7737, 'eval_samples_per_second': 21.367, 'eval_steps_per_second': 10.684, 'epoch': 4.12}
10/24/2023 13:12:45 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2600
10/24/2023 13:12:45 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.


























































































 86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 2699/3155 [1:32:04<14:07,  1.86s/it]




























































































 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                | 2799/3155 [1:35:07<10:57,  1.85s/it]
 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                | 2800/3155 [1:35:09<10:52,  1.84s/it][INFO|trainer.py:3081] 2023-10-24 13:18:54,409 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-24 13:18:54,409 >>   Num examples = 102
[INFO|trainer.py:3086] 2023-10-24 13:18:54,409 >>   Batch size = 1

 76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                   | 39/51 [00:03<00:00, 12.90it/s]
{'eval_loss': 1.094979166984558, 'eval_runtime': 4.8431, 'eval_samples_per_second': 21.061, 'eval_steps_per_second': 10.531, 'epoch': 4.44}
10/24/2023 13:18:59 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2800
[INFO|tokenization_utils_base.py:2217] 2023-10-24 13:18:59,481 >> Special tokens file saved in sft_model_path/checkpoint-2800/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-24 13:18:59,916 >> Deleting older checkpoint [sft_model_path/checkpoint-2600] due to args.save_total_limit

































































































 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏           | 2899/3155 [1:38:35<08:44,  2.05s/it]



































































































 95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉       | 3000/3155 [1:42:02<05:14,  2.03s/it][INFO|trainer.py:3081] 2023-10-24 13:25:46,983 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-24 13:25:46,983 >>   Num examples = 102
[INFO|trainer.py:3086] 2023-10-24 13:25:46,983 >>   Batch size = 1
{'loss': 1.0664, 'learning_rate': 6.31747632695695e-08, 'epoch': 4.75}

 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                 | 45/51 [00:03<00:00, 12.95it/s]
{'eval_loss': 1.0949169397354126, 'eval_runtime': 4.7899, 'eval_samples_per_second': 21.295, 'eval_steps_per_second': 10.647, 'epoch': 4.75}
10/24/2023 13:25:51 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-3000
[INFO|tokenization_utils_base.py:2217] 2023-10-24 13:25:52,007 >> Special tokens file saved in sft_model_path/checkpoint-3000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-24 13:25:52,451 >> Deleting older checkpoint [sft_model_path/checkpoint-2800] due to args.save_total_limit






























































































 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 3100/3155 [1:45:20<01:42,  1.87s/it]


















































100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3155/3155 [1:47:02<00:00,  1.84s/it][INFO|trainer.py:1934] 2023-10-24 13:30:47,207 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3155/3155 [1:47:02<00:00,  2.04s/it]
[INFO|tokenization_utils_base.py:2210] 2023-10-24 13:30:47,518 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-10-24 13:30:47,519 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-10-24 13:30:47,527 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-24 13:30:47,528 >>   Num examples = 102
[INFO|trainer.py:3086] 2023-10-24 13:30:47,528 >>   Batch size = 1
{'train_runtime': 6432.83, 'train_samples_per_second': 7.846, 'train_steps_per_second': 0.49, 'train_loss': 1.1372177775423802, 'epoch': 5.0}
10/24/2023 13:30:47 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
10/24/2023 13:30:47 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     1.1372
  train_runtime            = 1:47:12.82
  train_samples_per_second =      7.846
  train_steps_per_second   =       0.49

 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏           | 47/51 [00:03<00:00, 13.25it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_loss               =     1.0949
  eval_runtime            = 0:00:04.71
  eval_samples_per_second =     21.622
  eval_steps_per_second   =     10.811

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:03<00:00, 12.91it/s]