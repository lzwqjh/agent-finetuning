

  0%|                                                                                                                                       | 1/3045 [00:03<3:11:34,  3.78s/it]


































































































  3%|████▎                                                                                                                                 | 99/3045 [03:51<1:55:58,  2.36s/it]




































































































  7%|████████▋                                                                                                                            | 200/3045 [07:37<1:41:15,  2.14s/it]




































































































 10%|█████████████                                                                                                                        | 300/3045 [11:20<1:45:50,  2.31s/it]




































































































 13%|█████████████████▍                                                                                                                   | 400/3045 [15:00<1:43:19,  2.34s/it]


































































































 16%|█████████████████████▊                                                                                                               | 500/3045 [18:42<1:34:57,  2.24s/it][INFO|trainer.py:3081] 2023-12-03 19:32:25,583 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-03 19:32:25,583 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-03 19:32:25,583 >>   Batch size = 1
{'loss': 1.2179, 'learning_rate': 9.536335514258291e-06, 'epoch': 0.82}

[INFO|tokenization_utils_base.py:2217] 2023-12-03 19:32:30,884 >> Special tokens file saved in sft_model_path/checkpoint-500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-03 19:32:31,356 >> Deleting older checkpoint [sft_model_path/checkpoint-12000] due to args.save_total_limit
{'eval_loss': 1.1619104146957397, 'eval_runtime': 5.0423, 'eval_samples_per_second': 19.634, 'eval_steps_per_second': 9.916, 'epoch': 0.82}
12/03/2023 19:32:30 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-500
12/03/2023 19:32:30 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



































































































 20%|██████████████████████████▏                                                                                                          | 600/3045 [22:28<1:28:31,  2.17s/it]


































































































 23%|██████████████████████████████▌                                                                                                      | 699/3045 [26:10<1:28:04,  2.25s/it]





































































































 26%|██████████████████████████████████▉                                                                                                  | 800/3045 [30:20<1:27:48,  2.35s/it]



































































































 30%|███████████████████████████████████████▎                                                                                             | 899/3045 [34:07<1:18:23,  2.19s/it]




































































































 33%|███████████████████████████████████████████▋                                                                                         | 999/3045 [37:47<1:18:51,  2.31s/it]
 33%|███████████████████████████████████████████▎                                                                                        | 1000/3045 [37:49<1:18:45,  2.31s/it][INFO|trainer.py:3081] 2023-12-03 19:51:33,190 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-03 19:51:33,190 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-03 19:51:33,190 >>   Batch size = 1


 54%|██████████████████████████████████████████████████████████████████████████▌                                                               | 27/50 [00:02<00:01, 11.65it/s]
{'eval_loss': 1.131516695022583, 'eval_runtime': 5.3757, 'eval_samples_per_second': 18.416, 'eval_steps_per_second': 9.301, 'epoch': 1.64}
12/03/2023 19:51:38 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1000
 33%|███████████████████████████████████████████▎                                                                                        | 1000/3045 [37:55<1:18:45,  2.31s/it][INFO|tokenization_utils_base.py:2210] 2023-12-03 19:51:38,845 >> tokenizer config file saved in sft_model_path/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-03 19:51:38,846 >> Special tokens file saved in sft_model_path/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-03 19:51:39,323 >> Deleting older checkpoint [sft_model_path/checkpoint-500] due to args.save_total_limit


































































































 36%|███████████████████████████████████████████████▋                                                                                    | 1099/3045 [41:39<1:19:15,  2.44s/it]





































































































 39%|████████████████████████████████████████████████████                                                                                | 1200/3045 [45:33<1:08:21,  2.22s/it]




































































































 43%|████████████████████████████████████████████████████████▎                                                                           | 1300/3045 [49:11<1:03:47,  2.19s/it]




































































































 46%|█████████████████████████████████████████████████████████████▌                                                                        | 1400/3045 [52:46<57:56,  2.11s/it]



































































































 49%|██████████████████████████████████████████████████████████████████                                                                    | 1500/3045 [56:29<55:20,  2.15s/it][INFO|trainer.py:3081] 2023-12-03 20:10:13,111 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-03 20:10:13,111 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-03 20:10:13,112 >>   Batch size = 1
 18%|█████████████████████████                                                                                                                  | 9/50 [00:00<00:03, 12.62it/s]


[INFO|tokenization_utils_base.py:2217] 2023-12-03 20:10:18,420 >> Special tokens file saved in sft_model_path/checkpoint-1500/special_tokens_map.json
{'eval_loss': 1.113870620727539, 'eval_runtime': 5.0513, 'eval_samples_per_second': 19.599, 'eval_steps_per_second': 9.898, 'epoch': 2.46}
12/03/2023 20:10:18 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1500
12/03/2023 20:10:18 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2894] 2023-12-03 20:10:18,906 >> Deleting older checkpoint [sft_model_path/checkpoint-1000] due to args.save_total_limit


































































































 53%|█████████████████████████████████████████████████████████████████████▎                                                              | 1599/3045 [1:00:09<52:14,  2.17s/it]





































































































 56%|█████████████████████████████████████████████████████████████████████████▋                                                          | 1700/3045 [1:03:48<49:26,  2.21s/it]



































































































 59%|█████████████████████████████████████████████████████████████████████████████▉                                                      | 1799/3045 [1:07:23<44:41,  2.15s/it]





































































































 62%|██████████████████████████████████████████████████████████████████████████████████▎                                                 | 1900/3045 [1:11:03<41:07,  2.16s/it]



































































































 66%|██████████████████████████████████████████████████████████████████████████████████████▋                                             | 2000/3045 [1:14:44<38:32,  2.21s/it][INFO|trainer.py:3081] 2023-12-03 20:28:27,293 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-03 20:28:27,293 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-03 20:28:27,293 >>   Batch size = 1
{'loss': 1.1363, 'learning_rate': 2.7844824413191907e-06, 'epoch': 3.28}

[INFO|tokenization_utils_base.py:2217] 2023-12-03 20:28:32,562 >> Special tokens file saved in sft_model_path/checkpoint-2000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-03 20:28:33,046 >> Deleting older checkpoint [sft_model_path/checkpoint-1500] due to args.save_total_limit
{'eval_loss': 1.1062159538269043, 'eval_runtime': 5.0107, 'eval_samples_per_second': 19.758, 'eval_steps_per_second': 9.979, 'epoch': 3.28}
12/03/2023 20:28:32 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2000
12/03/2023 20:28:32 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



































































































 69%|███████████████████████████████████████████████████████████████████████████████████████████                                         | 2100/3045 [1:18:27<33:47,  2.15s/it]




































































































 72%|███████████████████████████████████████████████████████████████████████████████████████████████▎                                    | 2200/3045 [1:22:04<30:22,  2.16s/it]




































































































 76%|███████████████████████████████████████████████████████████████████████████████████████████████████▋                                | 2300/3045 [1:25:40<26:33,  2.14s/it]




































































































 79%|████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 2400/3045 [1:29:18<23:36,  2.20s/it]



































































































 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                       | 2499/3045 [1:33:04<19:47,  2.18s/it]
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                       | 2500/3045 [1:33:07<19:39,  2.16s/it][INFO|trainer.py:3081] 2023-12-03 20:46:50,422 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-03 20:46:50,423 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-03 20:46:50,423 >>   Batch size = 1


 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 49/50 [00:04<00:00, 11.89it/s]
{'eval_loss': 1.103017807006836, 'eval_runtime': 5.2542, 'eval_samples_per_second': 18.842, 'eval_steps_per_second': 9.516, 'epoch': 4.1}
12/03/2023 20:46:55 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2500
[INFO|tokenization_utils_base.py:2217] 2023-12-03 20:46:55,935 >> Special tokens file saved in sft_model_path/checkpoint-2500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-03 20:46:56,419 >> Deleting older checkpoint [sft_model_path/checkpoint-2000] due to args.save_total_limit


































































































 85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                   | 2599/3045 [1:36:47<15:50,  2.13s/it]



































































































 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████               | 2699/3045 [1:40:21<12:25,  2.15s/it]




































































































 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎          | 2799/3045 [1:43:56<08:43,  2.13s/it]





































































































 95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2900/3045 [1:47:33<05:12,  2.15s/it]



































































































 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 3000/3045 [1:51:10<01:36,  2.14s/it][INFO|trainer.py:3081] 2023-12-03 21:04:53,951 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-03 21:04:53,951 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-03 21:04:53,951 >>   Batch size = 1
{'loss': 1.1296, 'learning_rate': 5.72868495315293e-09, 'epoch': 4.92}

[INFO|tokenization_utils_base.py:2217] 2023-12-03 21:04:59,223 >> Special tokens file saved in sft_model_path/checkpoint-3000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-03 21:04:59,705 >> Deleting older checkpoint [sft_model_path/checkpoint-2500] due to args.save_total_limit
{'eval_loss': 1.1028066873550415, 'eval_runtime': 5.009, 'eval_samples_per_second': 19.764, 'eval_steps_per_second': 9.982, 'epoch': 4.92}
12/03/2023 21:04:58 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-3000
12/03/2023 21:04:58 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.











































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3045/3045 [1:52:54<00:00,  2.08s/it][INFO|trainer.py:1934] 2023-12-03 21:06:37,865 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3045/3045 [1:52:54<00:00,  2.22s/it]
[INFO|tokenization_utils_base.py:2210] 2023-12-03 21:06:38,215 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-03 21:06:38,216 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-12-03 21:06:38,226 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-03 21:06:38,226 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-03 21:06:38,226 >>   Batch size = 1
{'train_runtime': 6784.5055, 'train_samples_per_second': 7.183, 'train_steps_per_second': 0.449, 'train_loss': 1.208025509187545, 'epoch': 5.0}
12/03/2023 21:06:37 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
12/03/2023 21:06:37 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =        5.0
  train_loss               =      1.208
  train_runtime            = 1:53:04.50
  train_samples_per_second =      7.183
  train_steps_per_second   =      0.449


100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.83it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_loss               =     1.1028
  eval_runtime            = 0:00:05.03
  eval_samples_per_second =     19.646
  eval_steps_per_second   =      9.922
  perplexity              =     3.0127