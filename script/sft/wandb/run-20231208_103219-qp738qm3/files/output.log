

  0%|▏                                                                                                                                                                | 1/1160 [00:04<1:17:45,  4.03s/it]



































































































  9%|█████████████▉                                                                                                                                                   | 100/1160 [03:52<40:10,  2.27s/it]




































































































 17%|███████████████████████████▊                                                                                                                                     | 200/1160 [07:42<36:50,  2.30s/it]




































































































 26%|█████████████████████████████████████████▋                                                                                                                       | 300/1160 [11:34<33:24,  2.33s/it]




































































































 34%|███████████████████████████████████████████████████████▌                                                                                                         | 400/1160 [15:29<29:15,  2.31s/it]






























































































 43%|█████████████████████████████████████████████████████████████████████▎                                                                                           | 499/1160 [19:20<26:07,  2.37s/it]
 43%|█████████████████████████████████████████████████████████████████████▍                                                                                           | 500/1160 [19:22<26:17,  2.39s/it][INFO|trainer.py:3081] 2023-12-08 10:51:51,305 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-08 10:51:51,305 >>   Num examples = 38
[INFO|trainer.py:3086] 2023-12-08 10:51:51,305 >>   Batch size = 1
[INFO|tokenization_utils_base.py:2217] 2023-12-08 10:51:54,466 >> Special tokens file saved in sft_model_path/checkpoint-500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-08 10:51:54,963 >> Deleting older checkpoint [sft_model_path/checkpoint-9500] due to args.save_total_limit
{'eval_loss': 0.4852398931980133, 'eval_runtime': 2.8277, 'eval_samples_per_second': 13.438, 'eval_steps_per_second': 6.719, 'epoch': 2.15}
12/08/2023 10:51:54 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-500
12/08/2023 10:51:54 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



























































































 52%|███████████████████████████████████████████████████████████████████████████████████▎                                                                             | 600/1160 [23:20<21:28,  2.30s/it]




































































































 60%|█████████████████████████████████████████████████████████████████████████████████████████████████▏                                                               | 700/1160 [27:15<19:12,  2.50s/it]



































































































 69%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                  | 799/1160 [31:07<14:20,  2.38s/it]





































































































 78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                    | 900/1160 [35:04<10:19,  2.38s/it]



































































































 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                      | 999/1160 [38:55<06:10,  2.30s/it]
 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                      | 1000/1160 [38:57<06:06,  2.29s/it][INFO|trainer.py:3081] 2023-12-08 11:11:25,695 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-08 11:11:25,695 >>   Num examples = 38
[INFO|trainer.py:3086] 2023-12-08 11:11:25,696 >>   Batch size = 1
[INFO|tokenization_utils_base.py:2217] 2023-12-08 11:11:28,822 >> Special tokens file saved in sft_model_path/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-08 11:11:29,323 >> Deleting older checkpoint [sft_model_path/checkpoint-500] due to args.save_total_limit
{'eval_loss': 0.4332992732524872, 'eval_runtime': 2.8542, 'eval_samples_per_second': 13.314, 'eval_steps_per_second': 6.657, 'epoch': 4.3}
12/08/2023 11:11:28 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1000
12/08/2023 11:11:28 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



































































































 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋        | 1100/1160 [42:51<02:18,  2.30s/it]



























































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1160/1160 [45:09<00:00,  2.33s/it][INFO|trainer.py:1934] 2023-12-08 11:17:37,574 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1160/1160 [45:09<00:00,  2.34s/it]
[INFO|tokenization_utils_base.py:2210] 2023-12-08 11:17:37,925 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-08 11:17:37,925 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-12-08 11:17:37,934 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-08 11:17:37,934 >>   Num examples = 38
[INFO|trainer.py:3086] 2023-12-08 11:17:37,935 >>   Batch size = 1
{'train_runtime': 2719.399, 'train_samples_per_second': 6.838, 'train_steps_per_second': 0.427, 'train_loss': 0.8205339596189302, 'epoch': 4.99}
12/08/2023 11:17:37 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
12/08/2023 11:17:37 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =       4.99
  train_loss               =     0.8205
  train_runtime            = 0:45:19.39
  train_samples_per_second =      6.838
  train_steps_per_second   =      0.427
 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                 | 17/19 [00:01<00:00, 11.37it/s]
***** eval metrics *****
  epoch                   =       4.99
  eval_loss               =      0.432
  eval_runtime            = 0:00:02.67
  eval_samples_per_second =     14.232
  eval_steps_per_second   =      7.116

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:01<00:00, 10.85it/s]