

  0%|                                                                                                                                                                 | 1/3770 [00:04<4:21:08,  4.16s/it]


































































































  3%|████▏                                                                                                                                                           | 99/3770 [03:43<2:20:56,  2.30s/it]





































































































  5%|████████▍                                                                                                                                                      | 200/3770 [07:28<2:10:44,  2.20s/it]




































































































  8%|████████████▋                                                                                                                                                  | 300/3770 [11:14<2:14:20,  2.32s/it]



































































































 11%|████████████████▊                                                                                                                                              | 399/3770 [14:56<2:10:56,  2.33s/it]




































































































 13%|█████████████████████                                                                                                                                          | 500/3770 [18:43<1:58:15,  2.17s/it][INFO|trainer.py:3081] 2023-12-06 14:42:32,863 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 14:42:32,863 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 14:42:32,863 >>   Batch size = 1
  0%|                                                                                                                                                                             | 0/31 [00:00<?, ?it/s]

[INFO|tokenization_utils_base.py:2217] 2023-12-06 14:42:37,188 >> Special tokens file saved in sft_model_path/checkpoint-500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-06 14:42:37,698 >> Deleting older checkpoint [sft_model_path/checkpoint-3500] due to args.save_total_limit
{'eval_loss': 0.5006787180900574, 'eval_runtime': 4.0466, 'eval_samples_per_second': 15.075, 'eval_steps_per_second': 7.661, 'epoch': 1.32}
12/06/2023 14:42:36 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-500
12/06/2023 14:42:36 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



































































































 16%|█████████████████████████▎                                                                                                                                     | 600/3770 [22:31<1:55:03,  2.18s/it]



































































































 19%|█████████████████████████████▌                                                                                                                                 | 700/3770 [26:11<1:57:06,  2.29s/it]




































































































 21%|█████████████████████████████████▋                                                                                                                             | 800/3770 [30:00<1:46:59,  2.16s/it]




































































































 24%|█████████████████████████████████████▉                                                                                                                         | 900/3770 [33:42<1:47:21,  2.24s/it]



































































































 27%|█████████████████████████████████████████▉                                                                                                                    | 1000/3770 [37:24<1:47:32,  2.33s/it][INFO|trainer.py:3081] 2023-12-06 15:01:14,117 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 15:01:14,118 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 15:01:14,118 >>   Batch size = 1
{'loss': 0.4296, 'learning_rate': 8.619568527465547e-06, 'epoch': 2.65}

 45%|██████████████████████████████████████████████████████████████████████████                                                                                          | 14/31 [00:01<00:01,  9.44it/s]
{'eval_loss': 0.4214789867401123, 'eval_runtime': 4.8833, 'eval_samples_per_second': 12.492, 'eval_steps_per_second': 6.348, 'epoch': 2.65}
12/06/2023 15:01:19 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1000
 27%|█████████████████████████████████████████▉                                                                                                                    | 1000/3770 [37:29<1:47:32,  2.33s/it][INFO|tokenization_utils_base.py:2210] 2023-12-06 15:01:19,282 >> tokenizer config file saved in sft_model_path/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-06 15:01:19,283 >> Special tokens file saved in sft_model_path/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-06 15:01:19,806 >> Deleting older checkpoint [sft_model_path/checkpoint-500] due to args.save_total_limit



































































































 29%|██████████████████████████████████████████████                                                                                                                | 1100/3770 [41:13<1:42:59,  2.31s/it]




































































































 32%|██████████████████████████████████████████████████▎                                                                                                           | 1200/3770 [44:54<1:32:55,  2.17s/it]




































































































 34%|██████████████████████████████████████████████████████▍                                                                                                       | 1300/3770 [48:34<1:29:21,  2.17s/it]



































































































 37%|██████████████████████████████████████████████████████████▋                                                                                                   | 1399/3770 [52:13<1:25:56,  2.17s/it]




































































































 40%|██████████████████████████████████████████████████████████████▊                                                                                               | 1499/3770 [55:54<1:21:50,  2.16s/it]
 40%|██████████████████████████████████████████████████████████████▊                                                                                               | 1500/3770 [55:56<1:23:10,  2.20s/it][INFO|trainer.py:3081] 2023-12-06 15:19:45,842 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 15:19:45,842 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 15:19:45,842 >>   Batch size = 1

 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋     | 30/31 [00:02<00:00, 11.84it/s]
{'eval_loss': 0.38674721121788025, 'eval_runtime': 4.2342, 'eval_samples_per_second': 14.406, 'eval_steps_per_second': 7.321, 'epoch': 3.97}
12/06/2023 15:19:50 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1500
[INFO|tokenization_utils_base.py:2217] 2023-12-06 15:19:50,320 >> Special tokens file saved in sft_model_path/checkpoint-1500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-06 15:19:50,760 >> Deleting older checkpoint [sft_model_path/checkpoint-1000] due to args.save_total_limit



































































































 42%|███████████████████████████████████████████████████████████████████                                                                                           | 1600/3770 [59:42<1:15:52,  2.10s/it]



































































































 45%|██████████████████████████████████████████████████████████████████████▎                                                                                     | 1699/3770 [1:03:17<1:15:45,  2.20s/it]




































































































 48%|██████████████████████████████████████████████████████████████████████████▍                                                                                 | 1799/3770 [1:06:52<1:09:42,  2.12s/it]





































































































 50%|██████████████████████████████████████████████████████████████████████████████▌                                                                             | 1900/3770 [1:10:31<1:08:53,  2.21s/it]



































































































 53%|██████████████████████████████████████████████████████████████████████████████████▊                                                                         | 2000/3770 [1:14:06<1:04:04,  2.17s/it][INFO|trainer.py:3081] 2023-12-06 15:37:55,583 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 15:37:55,583 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 15:37:55,583 >>   Batch size = 1
{'loss': 0.3026, 'learning_rate': 4.750906798709381e-06, 'epoch': 5.3}

[INFO|tokenization_utils_base.py:2217] 2023-12-06 15:37:59,604 >> Special tokens file saved in sft_model_path/checkpoint-2000/special_tokens_map.json
{'eval_loss': 0.3627316653728485, 'eval_runtime': 3.7885, 'eval_samples_per_second': 16.101, 'eval_steps_per_second': 8.183, 'epoch': 5.3}
12/06/2023 15:37:59 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2000
12/06/2023 15:37:59 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2894] 2023-12-06 15:38:00,035 >> Deleting older checkpoint [sft_model_path/checkpoint-1500] due to args.save_total_limit



































































































 56%|████████████████████████████████████████████████████████████████████████████████████████                                                                      | 2100/3770 [1:17:48<58:55,  2.12s/it]




































































































 58%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                 | 2200/3770 [1:21:25<58:05,  2.22s/it]



































































































 61%|████████████████████████████████████████████████████████████████████████████████████████████████▎                                                             | 2299/3770 [1:24:59<51:53,  2.12s/it]




































































































 64%|████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 2399/3770 [1:28:36<47:18,  2.07s/it]




































































































 66%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                     | 2500/3770 [1:32:15<51:10,  2.42s/it][INFO|trainer.py:3081] 2023-12-06 15:56:04,527 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 15:56:04,528 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 15:56:04,528 >>   Batch size = 1
{'loss': 0.2771, 'learning_rate': 2.6933744369147265e-06, 'epoch': 6.62}

 29%|███████████████████████████████████████████████▉                                                                                                                     | 9/31 [00:00<00:01, 13.43it/s]
{'eval_loss': 0.34491071105003357, 'eval_runtime': 4.0709, 'eval_samples_per_second': 14.984, 'eval_steps_per_second': 7.615, 'epoch': 6.62}
12/06/2023 15:56:08 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2500
 66%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                     | 2500/3770 [1:32:19<51:10,  2.42s/it][INFO|tokenization_utils_base.py:2210] 2023-12-06 15:56:08,867 >> tokenizer config file saved in sft_model_path/checkpoint-2500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-06 15:56:08,868 >> Special tokens file saved in sft_model_path/checkpoint-2500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-06 15:56:09,309 >> Deleting older checkpoint [sft_model_path/checkpoint-2000] due to args.save_total_limit


































































































 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                 | 2599/3770 [1:36:10<41:55,  2.15s/it]





































































































 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                            | 2700/3770 [1:40:02<39:32,  2.22s/it]




































































































 74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 2800/3770 [1:43:39<35:07,  2.17s/it]




































































































 77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                    | 2900/3770 [1:47:34<31:33,  2.18s/it]



































































































 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                | 3000/3770 [1:51:14<28:41,  2.24s/it][INFO|trainer.py:3081] 2023-12-06 16:15:03,890 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 16:15:03,890 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 16:15:03,891 >>   Batch size = 1
 16%|██████████████████████████▌                                                                                                                                          | 5/31 [00:00<00:01, 14.12it/s]

[INFO|tokenization_utils_base.py:2217] 2023-12-06 16:15:08,078 >> Special tokens file saved in sft_model_path/checkpoint-3000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-06 16:15:08,575 >> Deleting older checkpoint [sft_model_path/checkpoint-2500] due to args.save_total_limit
{'eval_loss': 0.3388219475746155, 'eval_runtime': 3.9199, 'eval_samples_per_second': 15.562, 'eval_steps_per_second': 7.908, 'epoch': 7.95}
12/06/2023 16:15:07 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-3000
12/06/2023 16:15:07 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.


































































































 82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                            | 3099/3770 [1:54:50<23:42,  2.12s/it]




































































































 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                        | 3199/3770 [1:58:19<19:34,  2.06s/it]




































































































 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 3299/3770 [2:01:44<16:13,  2.07s/it]




































































































 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍               | 3399/3770 [2:05:09<14:21,  2.32s/it]





































































































 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋           | 3500/3770 [2:08:40<09:23,  2.09s/it]
 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋           | 3500/3770 [2:08:40<09:23,  2.09s/it][INFO|trainer.py:3081] 2023-12-06 16:32:29,715 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 16:32:29,716 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 16:32:29,716 >>   Batch size = 1

[INFO|tokenization_utils_base.py:2217] 2023-12-06 16:32:33,664 >> Special tokens file saved in sft_model_path/checkpoint-3500/special_tokens_map.json
{'eval_loss': 0.34034913778305054, 'eval_runtime': 3.7223, 'eval_samples_per_second': 16.388, 'eval_steps_per_second': 8.328, 'epoch': 9.27}
12/06/2023 16:32:33 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-3500
12/06/2023 16:32:33 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2894] 2023-12-06 16:32:34,090 >> Deleting older checkpoint [sft_model_path/checkpoint-3000] due to args.save_total_limit



































































































 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉       | 3600/3770 [2:12:08<05:49,  2.06s/it]




































































































 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 3700/3770 [2:15:38<02:30,  2.15s/it]





































































100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3770/3770 [2:18:01<00:00,  2.02s/it][INFO|trainer.py:1934] 2023-12-06 16:41:51,326 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3770/3770 [2:18:01<00:00,  2.20s/it]
{'train_runtime': 8292.1271, 'train_samples_per_second': 7.283, 'train_steps_per_second': 0.455, 'train_loss': 0.46178022088675663, 'epoch': 9.99}
12/06/2023 16:41:51 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
12/06/2023 16:41:51 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2210] 2023-12-06 16:41:51,672 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-06 16:41:51,672 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-12-06 16:41:51,681 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 16:41:51,681 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 16:41:51,681 >>   Batch size = 1
 26%|██████████████████████████████████████████▌                                                                                                                          | 8/31 [00:00<00:01, 14.43it/s]
***** train metrics *****
  epoch                    =       9.99
  train_loss               =     0.4618
  train_runtime            = 2:18:12.12
  train_samples_per_second =      7.283

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 12.38it/s]
***** eval metrics *****
  epoch                   =       9.99
  eval_loss               =     0.3396
  eval_runtime            = 0:00:03.66
  eval_samples_per_second =     16.626
  eval_steps_per_second   =      8.449
  perplexity              =     1.4043