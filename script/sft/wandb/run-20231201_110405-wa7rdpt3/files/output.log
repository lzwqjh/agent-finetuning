
  0%|                                                                                                                                                 | 0/6080 [00:00<?, ?it/s]


































































































  2%|██▏                                                                                                                                   | 99/6080 [03:34<3:27:41,  2.08s/it]





































































































  3%|████▍                                                                                                                                | 200/6080 [07:06<3:26:30,  2.11s/it]




































































































  5%|██████▌                                                                                                                              | 300/6080 [10:39<3:21:04,  2.09s/it]



































































































  7%|████████▋                                                                                                                            | 399/6080 [14:06<3:17:33,  2.09s/it]




































































































  8%|██████████▉                                                                                                                          | 499/6080 [17:36<3:17:31,  2.12s/it]
  8%|██████████▉                                                                                                                          | 500/6080 [17:38<3:16:45,  2.12s/it][INFO|trainer.py:3081] 2023-12-01 11:21:52,335 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-01 11:21:52,335 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-01 11:21:52,335 >>   Batch size = 1
[INFO|tokenization_utils_base.py:2217] 2023-12-01 11:21:55,726 >> Special tokens file saved in sft_model_path/checkpoint-500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-01 11:21:56,179 >> Deleting older checkpoint [sft_model_path/checkpoint-2000] due to args.save_total_limit
{'eval_loss': 1.1036412715911865, 'eval_runtime': 3.147, 'eval_samples_per_second': 31.458, 'eval_steps_per_second': 7.944, 'epoch': 1.64}
12/01/2023 11:21:55 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-500
12/01/2023 11:21:55 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



































































































 10%|█████████████▏                                                                                                                       | 600/6080 [21:22<3:30:20,  2.30s/it]




































































































 12%|███████████████▎                                                                                                                     | 700/6080 [25:07<3:24:47,  2.28s/it]




































































































 13%|█████████████████▌                                                                                                                   | 800/6080 [28:53<3:19:50,  2.27s/it]



































































































 15%|███████████████████▋                                                                                                                 | 899/6080 [32:38<3:33:54,  2.48s/it]




































































































 16%|█████████████████████▊                                                                                                               | 999/6080 [36:15<3:00:31,  2.13s/it]
 16%|█████████████████████▋                                                                                                              | 1000/6080 [36:17<3:01:49,  2.15s/it][INFO|trainer.py:3081] 2023-12-01 11:40:31,040 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-01 11:40:31,041 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-01 11:40:31,041 >>   Batch size = 1

 12%|████████████████▋                                                                                                                          | 3/25 [00:00<00:01, 15.00it/s]
{'eval_loss': 1.0761569738388062, 'eval_runtime': 3.2285, 'eval_samples_per_second': 30.664, 'eval_steps_per_second': 7.744, 'epoch': 3.28}
12/01/2023 11:40:34 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1000
 16%|█████████████████████▋                                                                                                              | 1000/6080 [36:20<3:01:49,  2.15s/it][INFO|tokenization_utils_base.py:2210] 2023-12-01 11:40:34,521 >> tokenizer config file saved in sft_model_path/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-01 11:40:34,522 >> Special tokens file saved in sft_model_path/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-01 11:40:35,030 >> Deleting older checkpoint [sft_model_path/checkpoint-500] due to args.save_total_limit


































































































 18%|███████████████████████▊                                                                                                            | 1099/6080 [39:51<3:12:36,  2.32s/it]




































































































 20%|██████████████████████████                                                                                                          | 1200/6080 [43:25<2:47:59,  2.07s/it]




































































































 21%|████████████████████████████▏                                                                                                       | 1300/6080 [46:57<2:45:13,  2.07s/it]


































































































 23%|██████████████████████████████▎                                                                                                     | 1399/6080 [50:25<2:42:32,  2.08s/it]




































































































 25%|████████████████████████████████▌                                                                                                   | 1500/6080 [53:56<2:41:05,  2.11s/it][INFO|trainer.py:3081] 2023-12-01 11:58:09,929 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-01 11:58:09,929 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-01 11:58:09,929 >>   Batch size = 1
  0%|                                                                                                                                                   | 0/25 [00:00<?, ?it/s]

 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉           | 23/25 [00:01<00:00, 12.38it/s]
{'eval_loss': 1.063878059387207, 'eval_runtime': 3.0933, 'eval_samples_per_second': 32.004, 'eval_steps_per_second': 8.082, 'epoch': 4.92}
12/01/2023 11:58:13 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1500
[INFO|tokenization_utils_base.py:2217] 2023-12-01 11:58:13,263 >> Special tokens file saved in sft_model_path/checkpoint-1500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-01 11:58:13,707 >> Deleting older checkpoint [sft_model_path/checkpoint-1000] due to args.save_total_limit



































































































 26%|██████████████████████████████████▋                                                                                                 | 1600/6080 [57:29<2:35:13,  2.08s/it]




































































































 28%|████████████████████████████████████▎                                                                                             | 1700/6080 [1:00:58<2:32:09,  2.08s/it]




































































































 30%|██████████████████████████████████████▍                                                                                           | 1800/6080 [1:04:32<2:43:52,  2.30s/it]



































































































 31%|████████████████████████████████████████▌                                                                                         | 1899/6080 [1:08:01<2:26:40,  2.10s/it]




































































































 33%|██████████████████████████████████████████▊                                                                                       | 2000/6080 [1:11:43<2:35:59,  2.29s/it][INFO|trainer.py:3081] 2023-12-01 12:15:57,160 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-01 12:15:57,160 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-01 12:15:57,160 >>   Batch size = 1
{'loss': 1.0726, 'learning_rate': 7.836950165447742e-06, 'epoch': 6.57}
 68%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                            | 17/25 [00:01<00:00, 12.53it/s]
{'eval_loss': 1.0560914278030396, 'eval_runtime': 3.1121, 'eval_samples_per_second': 31.811, 'eval_steps_per_second': 8.033, 'epoch': 6.57}
12/01/2023 12:16:00 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2000
[INFO|tokenization_utils_base.py:2217] 2023-12-01 12:16:00,512 >> Special tokens file saved in sft_model_path/checkpoint-2000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-01 12:16:00,956 >> Deleting older checkpoint [sft_model_path/checkpoint-1500] due to args.save_total_limit



































































































 35%|████████████████████████████████████████████▉                                                                                     | 2100/6080 [1:15:15<2:18:30,  2.09s/it]



































































































 36%|███████████████████████████████████████████████                                                                                   | 2199/6080 [1:18:43<2:15:36,  2.10s/it]





































































































 38%|█████████████████████████████████████████████████▏                                                                                | 2300/6080 [1:22:14<2:11:58,  2.09s/it]




































































































 39%|███████████████████████████████████████████████████▎                                                                              | 2400/6080 [1:25:43<2:09:45,  2.12s/it]



































































































 41%|█████████████████████████████████████████████████████▍                                                                            | 2499/6080 [1:29:12<2:03:09,  2.06s/it]
 41%|█████████████████████████████████████████████████████▍                                                                            | 2500/6080 [1:29:14<2:02:49,  2.06s/it][INFO|trainer.py:3081] 2023-12-01 12:33:27,967 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-01 12:33:27,967 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-01 12:33:27,967 >>   Batch size = 1

[INFO|tokenization_utils_base.py:2217] 2023-12-01 12:33:31,400 >> Special tokens file saved in sft_model_path/checkpoint-2500/special_tokens_map.json
{'eval_loss': 1.0567454099655151, 'eval_runtime': 3.1933, 'eval_samples_per_second': 31.002, 'eval_steps_per_second': 7.829, 'epoch': 8.21}
12/01/2023 12:33:31 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2500
12/01/2023 12:33:31 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2894] 2023-12-01 12:33:31,854 >> Deleting older checkpoint [sft_model_path/checkpoint-2000] due to args.save_total_limit



































































































 43%|███████████████████████████████████████████████████████▌                                                                          | 2600/6080 [1:32:42<1:59:01,  2.05s/it]




































































































 44%|█████████████████████████████████████████████████████████▋                                                                        | 2700/6080 [1:36:08<1:55:46,  2.06s/it]



































































































 46%|███████████████████████████████████████████████████████████▊                                                                      | 2799/6080 [1:39:33<1:53:52,  2.08s/it]




































































































 48%|█████████████████████████████████████████████████████████████▉                                                                    | 2899/6080 [1:43:06<2:04:16,  2.34s/it]




































































































 49%|████████████████████████████████████████████████████████████████                                                                  | 2999/6080 [1:46:37<1:45:11,  2.05s/it]
 49%|████████████████████████████████████████████████████████████████▏                                                                 | 3000/6080 [1:46:39<1:45:24,  2.05s/it][INFO|trainer.py:3081] 2023-12-01 12:50:53,342 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-01 12:50:53,342 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-01 12:50:53,342 >>   Batch size = 1
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉           | 23/25 [00:01<00:00, 11.88it/s]
{'eval_loss': 1.0592739582061768, 'eval_runtime': 3.1775, 'eval_samples_per_second': 31.157, 'eval_steps_per_second': 7.868, 'epoch': 9.85}
12/01/2023 12:50:56 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-3000
[INFO|tokenization_utils_base.py:2217] 2023-12-01 12:50:56,756 >> Special tokens file saved in sft_model_path/checkpoint-3000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-01 12:50:57,199 >> Deleting older checkpoint [sft_model_path/checkpoint-2500] due to args.save_total_limit

































































































 51%|██████████████████████████████████████████████████████████████████▎                                                               | 3099/6080 [1:50:10<1:40:57,  2.03s/it]




































































































 53%|████████████████████████████████████████████████████████████████████▍                                                             | 3199/6080 [1:53:47<1:51:07,  2.31s/it]




































































































 54%|██████████████████████████████████████████████████████████████████████▌                                                           | 3299/6080 [1:57:21<1:39:06,  2.14s/it]




































































































 56%|████████████████████████████████████████████████████████████████████████▋                                                         | 3399/6080 [2:01:08<1:34:36,  2.12s/it]




































































































 58%|██████████████████████████████████████████████████████████████████████████▊                                                       | 3500/6080 [2:04:36<1:31:44,  2.13s/it][INFO|trainer.py:3081] 2023-12-01 13:08:50,212 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-01 13:08:50,212 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-01 13:08:50,213 >>   Batch size = 1
{'loss': 1.0081, 'learning_rate': 4.027324368127973e-06, 'epoch': 11.49}
[INFO|tokenization_utils_base.py:2217] 2023-12-01 13:08:53,552 >> Special tokens file saved in sft_model_path/checkpoint-3500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-01 13:08:53,997 >> Deleting older checkpoint [sft_model_path/checkpoint-3000] due to args.save_total_limit
{'eval_loss': 1.0659208297729492, 'eval_runtime': 3.0983, 'eval_samples_per_second': 31.953, 'eval_steps_per_second': 8.069, 'epoch': 11.49}
12/01/2023 13:08:53 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-3500
12/01/2023 13:08:53 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



































































































 59%|████████████████████████████████████████████████████████████████████████████▉                                                     | 3600/6080 [2:08:09<1:24:30,  2.04s/it]



































































































 61%|███████████████████████████████████████████████████████████████████████████████                                                   | 3700/6080 [2:11:38<1:26:24,  2.18s/it]


































































































 62%|█████████████████████████████████████████████████████████████████████████████████▏                                                | 3799/6080 [2:15:15<1:19:19,  2.09s/it]




































































































 64%|███████████████████████████████████████████████████████████████████████████████████▎                                              | 3899/6080 [2:18:41<1:14:46,  2.06s/it]




































































































 66%|█████████████████████████████████████████████████████████████████████████████████████▌                                            | 3999/6080 [2:22:15<1:11:30,  2.06s/it]
 66%|█████████████████████████████████████████████████████████████████████████████████████▌                                            | 4000/6080 [2:22:17<1:11:44,  2.07s/it][INFO|trainer.py:3081] 2023-12-01 13:26:30,898 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-01 13:26:30,898 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-01 13:26:30,899 >>   Batch size = 1
[INFO|tokenization_utils_base.py:2217] 2023-12-01 13:26:34,288 >> Special tokens file saved in sft_model_path/checkpoint-4000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-01 13:26:34,736 >> Deleting older checkpoint [sft_model_path/checkpoint-3500] due to args.save_total_limit
{'eval_loss': 1.072546362876892, 'eval_runtime': 3.1431, 'eval_samples_per_second': 31.497, 'eval_steps_per_second': 7.954, 'epoch': 13.13}
12/01/2023 13:26:34 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-4000
12/01/2023 13:26:34 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.


































































































 67%|███████████████████████████████████████████████████████████████████████████████████████▋                                          | 4099/6080 [2:25:45<1:08:38,  2.08s/it]




































































































 69%|█████████████████████████████████████████████████████████████████████████████████████████▊                                        | 4199/6080 [2:29:12<1:03:20,  2.02s/it]





































































































 71%|███████████████████████████████████████████████████████████████████████████████████████████▉                                      | 4300/6080 [2:32:42<1:00:29,  2.04s/it]



































































































 72%|██████████████████████████████████████████████████████████████████████████████████████████████                                    | 4399/6080 [2:36:10<1:03:25,  2.26s/it]




































































































 74%|█████████████████████████████████████████████████████████████████████████████████████████████████▋                                  | 4499/6080 [2:39:41<54:13,  2.06s/it]
 74%|█████████████████████████████████████████████████████████████████████████████████████████████████▋                                  | 4500/6080 [2:39:43<54:16,  2.06s/it][INFO|trainer.py:3081] 2023-12-01 13:43:56,907 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-01 13:43:56,908 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-01 13:43:56,908 >>   Batch size = 1
[INFO|tokenization_utils_base.py:2217] 2023-12-01 13:44:00,271 >> Special tokens file saved in sft_model_path/checkpoint-4500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-01 13:44:00,715 >> Deleting older checkpoint [sft_model_path/checkpoint-4000] due to args.save_total_limit
{'eval_loss': 1.0734643936157227, 'eval_runtime': 3.1289, 'eval_samples_per_second': 31.64, 'eval_steps_per_second': 7.99, 'epoch': 14.77}
12/01/2023 13:44:00 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-4500
12/01/2023 13:44:00 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



































































































 76%|███████████████████████████████████████████████████████████████████████████████████████████████████▊                                | 4600/6080 [2:43:25<51:09,  2.07s/it]




































































































 77%|██████████████████████████████████████████████████████████████████████████████████████████████████████                              | 4700/6080 [2:46:49<47:11,  2.05s/it]




































































































 79%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏                           | 4800/6080 [2:50:18<44:32,  2.09s/it]




































































































 81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▍                         | 4900/6080 [2:53:45<40:32,  2.06s/it]



































































































 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                       | 5000/6080 [2:57:11<37:09,  2.06s/it][INFO|trainer.py:3081] 2023-12-01 14:01:25,203 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-01 14:01:25,203 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-01 14:01:25,203 >>   Batch size = 1
{'loss': 0.9697, 'learning_rate': 8.064779122827654e-07, 'epoch': 16.41}
 60%|██████████████████████████████████████████████████████████████████████████████████▊                                                       | 15/25 [00:01<00:00, 12.61it/s]
{'eval_loss': 1.07610023021698, 'eval_runtime': 3.1344, 'eval_samples_per_second': 31.585, 'eval_steps_per_second': 7.976, 'epoch': 16.41}
12/01/2023 14:01:28 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-5000
[INFO|tokenization_utils_base.py:2217] 2023-12-01 14:01:28,582 >> Special tokens file saved in sft_model_path/checkpoint-5000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-01 14:01:29,027 >> Deleting older checkpoint [sft_model_path/checkpoint-4500] due to args.save_total_limit



































































































 84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                     | 5100/6080 [3:00:41<33:31,  2.05s/it]



































































































 86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                   | 5199/6080 [3:04:14<30:04,  2.05s/it]





































































































 87%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 5300/6080 [3:07:46<26:51,  2.07s/it]




































































































 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏              | 5400/6080 [3:11:14<23:02,  2.03s/it]



































































































 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍            | 5500/6080 [3:14:41<19:50,  2.05s/it][INFO|trainer.py:3081] 2023-12-01 14:18:55,037 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-01 14:18:55,038 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-01 14:18:55,038 >>   Batch size = 1
{'loss': 0.9745, 'learning_rate': 2.376072454733358e-07, 'epoch': 18.05}
[INFO|tokenization_utils_base.py:2217] 2023-12-01 14:18:58,327 >> Special tokens file saved in sft_model_path/checkpoint-5500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-01 14:18:58,778 >> Deleting older checkpoint [sft_model_path/checkpoint-5000] due to args.save_total_limit
{'eval_loss': 1.0773717164993286, 'eval_runtime': 3.0424, 'eval_samples_per_second': 32.54, 'eval_steps_per_second': 8.217, 'epoch': 18.05}
12/01/2023 14:18:58 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-5500
12/01/2023 14:18:58 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



































































































 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 5600/6080 [3:18:10<16:13,  2.03s/it]




































































































 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊        | 5700/6080 [3:21:34<12:49,  2.02s/it]


































































































 95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉      | 5799/6080 [3:24:53<09:29,  2.03s/it]





































































































 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████    | 5900/6080 [3:28:19<06:06,  2.03s/it]



































































































 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 6000/6080 [3:31:43<02:44,  2.06s/it][INFO|trainer.py:3081] 2023-12-01 14:35:57,669 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-01 14:35:57,669 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-01 14:35:57,669 >>   Batch size = 1
{'loss': 0.9712, 'learning_rate': 4.654576182215876e-09, 'epoch': 19.7}
 72%|███████████████████████████████████████████████████████████████████████████████████████████████████▎                                      | 18/25 [00:01<00:00, 12.26it/s]
{'eval_loss': 1.077605128288269, 'eval_runtime': 3.056, 'eval_samples_per_second': 32.395, 'eval_steps_per_second': 8.18, 'epoch': 19.7}
12/01/2023 14:36:00 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-6000
[INFO|tokenization_utils_base.py:2217] 2023-12-01 14:36:00,965 >> Special tokens file saved in sft_model_path/checkpoint-6000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-01 14:36:01,406 >> Deleting older checkpoint [sft_model_path/checkpoint-5500] due to args.save_total_limit













































































100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 6079/6080 [3:34:28<00:02,  2.04s/it]
{'train_runtime': 12881.5134, 'train_samples_per_second': 15.133, 'train_steps_per_second': 0.472, 'train_loss': 1.059009334327359, 'epoch': 19.96}
12/01/2023 14:38:44 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
12/01/2023 14:38:44 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =      19.96
  train_loss               =      1.059
  train_runtime            = 3:34:41.51
  train_samples_per_second =     15.133
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6080/6080 [3:34:31<00:00,  2.04s/it][INFO|trainer.py:1934] 2023-12-01 14:38:44,864 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6080/6080 [3:34:31<00:00,  2.12s/it]
[INFO|tokenization_utils_base.py:2210] 2023-12-01 14:38:45,160 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-01 14:38:45,160 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-12-01 14:38:45,172 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-01 14:38:45,172 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-01 14:38:45,172 >>   Batch size = 1

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:02<00:00, 11.59it/s]
***** eval metrics *****
  epoch                   =      19.96
  eval_loss               =     1.0776
  eval_runtime            = 0:00:02.99
  eval_samples_per_second =     33.083
  eval_steps_per_second   =      8.354
  perplexity              =     2.9377