

  0%|▏                                                                                                                                                                | 1/1165 [00:04<1:33:20,  4.81s/it]


































































































  8%|█████████████▊                                                                                                                                                    | 99/1165 [04:36<47:16,  2.66s/it]




































































































 17%|███████████████████████████▌                                                                                                                                     | 199/1165 [09:17<48:17,  3.00s/it]





































































































 26%|█████████████████████████████████████████▍                                                                                                                       | 300/1165 [14:03<41:36,  2.89s/it]




































































































 34%|███████████████████████████████████████████████████████▎                                                                                                         | 400/1165 [18:55<36:50,  2.89s/it]



































































































 43%|█████████████████████████████████████████████████████████████████████                                                                                            | 500/1165 [23:55<32:04,  2.89s/it][INFO|trainer.py:3081] 2023-12-06 21:36:14,454 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 21:36:14,455 >>   Num examples = 38
[INFO|trainer.py:3086] 2023-12-06 21:36:14,455 >>   Batch size = 1
{'loss': 0.3751, 'learning_rate': 6.372246903165445e-06, 'epoch': 2.14}


[INFO|tokenization_utils_base.py:2217] 2023-12-06 21:36:19,534 >> Special tokens file saved in sft_model_path/checkpoint-500/special_tokens_map.json
{'eval_loss': 0.3835354745388031, 'eval_runtime': 4.6637, 'eval_samples_per_second': 8.148, 'eval_steps_per_second': 4.074, 'epoch': 2.14}
12/06/2023 21:36:19 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-500
12/06/2023 21:36:19 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



































































































 52%|██████████████████████████████████████████████████████████████████████████████████▉                                                                              | 600/1165 [28:43<27:46,  2.95s/it]




































































































 60%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                | 700/1165 [33:22<21:53,  2.82s/it]



































































































 69%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                  | 799/1165 [38:03<15:43,  2.58s/it]




































































































 77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                    | 899/1165 [42:45<13:10,  2.97s/it]




































































































 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 999/1165 [47:57<07:48,  2.82s/it]
 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                      | 1000/1165 [48:00<07:53,  2.87s/it][INFO|trainer.py:3081] 2023-12-06 22:00:19,910 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 22:00:19,910 >>   Num examples = 38
[INFO|trainer.py:3086] 2023-12-06 22:00:19,910 >>   Batch size = 1


 42%|█████████████████████████████████████████████████████████████████████▍                                                                                               | 8/19 [00:02<00:03,  3.53it/s]
{'eval_loss': 0.3461661636829376, 'eval_runtime': 5.6979, 'eval_samples_per_second': 6.669, 'eval_steps_per_second': 3.335, 'epoch': 4.28}
12/06/2023 22:00:25 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1000
 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                      | 1000/1165 [48:06<07:53,  2.87s/it][INFO|tokenization_utils_base.py:2210] 2023-12-06 22:00:25,903 >> tokenizer config file saved in sft_model_path/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-06 22:00:25,903 >> Special tokens file saved in sft_model_path/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-06 22:00:26,434 >> Deleting older checkpoint [sft_model_path/checkpoint-500] due to args.save_total_limit



































































































 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████         | 1100/1165 [53:35<03:17,  3.03s/it]
































































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1165/1165 [56:45<00:00,  2.90s/it][INFO|trainer.py:1934] 2023-12-06 22:09:04,269 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1165/1165 [56:45<00:00,  2.92s/it]
{'train_runtime': 3416.0339, 'train_samples_per_second': 5.465, 'train_steps_per_second': 0.341, 'train_loss': 0.6845091381809743, 'epoch': 4.99}
12/06/2023 22:09:04 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
12/06/2023 22:09:04 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =       4.99
  train_loss               =     0.6845
  train_runtime            = 0:56:56.03
  train_samples_per_second =      5.465
  train_steps_per_second   =      0.341
[INFO|tokenization_utils_base.py:2210] 2023-12-06 22:09:04,829 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-06 22:09:04,830 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-12-06 22:09:04,840 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 22:09:04,840 >>   Num examples = 38
[INFO|trainer.py:3086] 2023-12-06 22:09:04,840 >>   Batch size = 1

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:02<00:00,  7.16it/s]
***** eval metrics *****
  epoch                   =       4.99
  eval_loss               =     0.3459
  eval_runtime            = 0:00:03.98
  eval_samples_per_second =      9.544
  eval_steps_per_second   =      4.772

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:02<00:00,  6.40it/s]