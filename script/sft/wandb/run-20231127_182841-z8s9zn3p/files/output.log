

  0%|                                                                                                                                                              | 1/2160 [00:03<2:18:54,  3.86s/it]
































































































  5%|███████▏                                                                                                                                                    | 100/2160 [03:34<1:16:10,  2.22s/it]




































































































  9%|██████████████▍                                                                                                                                             | 200/2160 [07:13<1:10:36,  2.16s/it]






























































































 14%|█████████████████████▌                                                                                                                                      | 299/2160 [10:29<1:00:40,  1.96s/it]


































































































 18%|████████████████████████████▊                                                                                                                               | 399/2160 [13:54<1:04:50,  2.21s/it]


































































































 23%|████████████████████████████████████▌                                                                                                                         | 499/2160 [17:28<55:38,  2.01s/it]
 23%|████████████████████████████████████▌                                                                                                                         | 500/2160 [17:30<54:55,  1.98s/it][INFO|trainer.py:3081] 2023-11-27 18:46:20,887 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-11-27 18:46:20,887 >>   Num examples = 24
[INFO|trainer.py:3086] 2023-11-27 18:46:20,887 >>   Batch size = 1
  0%|                                                                                                                                                                           | 0/6 [00:00<?, ?it/s]
{'eval_loss': 0.795079231262207, 'eval_runtime': 1.2766, 'eval_samples_per_second': 18.8, 'eval_steps_per_second': 4.7, 'epoch': 6.94}
11/27/2023 18:46:22 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-500

[INFO|tokenization_utils_base.py:2217] 2023-11-27 18:46:22,397 >> Special tokens file saved in sft_model_path/checkpoint-500/special_tokens_map.json






























































































 28%|███████████████████████████████████████████▉                                                                                                                  | 600/2160 [20:47<49:56,  1.92s/it]
































































































 32%|███████████████████████████████████████████████████▏                                                                                                          | 700/2160 [24:02<47:23,  1.95s/it]


































































































 37%|██████████████████████████████████████████████████████████▌                                                                                                   | 800/2160 [27:20<44:38,  1.97s/it]


































































































 42%|█████████████████████████████████████████████████████████████████▊                                                                                            | 900/2160 [30:44<41:28,  1.98s/it]


































































































 46%|████████████████████████████████████████████████████████████████████████▋                                                                                    | 1000/2160 [34:16<42:04,  2.18s/it][INFO|trainer.py:3081] 2023-11-27 19:03:06,713 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-11-27 19:03:06,713 >>   Num examples = 24
[INFO|trainer.py:3086] 2023-11-27 19:03:06,713 >>   Batch size = 1
 46%|████████████████████████████████████████████████████████████████████████▋                                                                                    | 1000/2160 [34:17<42:04,  2.18s/it][INFO|tokenization_utils_base.py:2210] 2023-11-27 19:03:08,190 >> tokenizer config file saved in sft_model_path/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-11-27 19:03:08,191 >> Special tokens file saved in sft_model_path/checkpoint-1000/special_tokens_map.json
{'loss': 0.4946, 'learning_rate': 5.839511021513853e-06, 'epoch': 13.89}
{'eval_loss': 0.6899229884147644, 'eval_runtime': 1.2333, 'eval_samples_per_second': 19.461, 'eval_steps_per_second': 4.865, 'epoch': 13.89}
11/27/2023 19:03:07 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1000
11/27/2023 19:03:07 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2894] 2023-11-27 19:03:08,645 >> Deleting older checkpoint [sft_model_path/checkpoint-500] due to args.save_total_limit































































































 51%|███████████████████████████████████████████████████████████████████████████████▉                                                                             | 1099/2160 [37:35<34:11,  1.93s/it]


































































































 56%|███████████████████████████████████████████████████████████████████████████████████████▏                                                                     | 1199/2160 [41:09<34:57,  2.18s/it]




































































































 60%|██████████████████████████████████████████████████████████████████████████████████████████████▍                                                              | 1299/2160 [44:46<32:11,  2.24s/it]


































































































 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                       | 1399/2160 [48:07<24:29,  1.93s/it]































































































 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                | 1499/2160 [51:21<21:13,  1.93s/it]
 69%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                | 1500/2160 [51:23<21:08,  1.92s/it][INFO|trainer.py:3081] 2023-11-27 19:20:13,639 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-11-27 19:20:13,640 >>   Num examples = 24
[INFO|trainer.py:3086] 2023-11-27 19:20:13,640 >>   Batch size = 1
 69%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                | 1500/2160 [51:24<21:08,  1.92s/it]
{'eval_loss': 0.6386430859565735, 'eval_runtime': 1.2925, 'eval_samples_per_second': 18.569, 'eval_steps_per_second': 4.642, 'epoch': 20.83}
11/27/2023 19:20:14 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1500
 69%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                | 1500/2160 [51:24<21:08,  1.92s/it][INFO|tokenization_utils_base.py:2210] 2023-11-27 19:20:15,168 >> tokenizer config file saved in sft_model_path/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-11-27 19:20:15,168 >> Special tokens file saved in sft_model_path/checkpoint-1500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-11-27 19:20:15,609 >> Deleting older checkpoint [sft_model_path/checkpoint-1000] due to args.save_total_limit































































































 74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 1600/2160 [54:42<18:21,  1.97s/it]



































































































 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 1700/2160 [58:12<16:25,  2.14s/it]


































































































 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                         | 1800/2160 [1:01:47<12:57,  2.16s/it]



































































































 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                  | 1900/2160 [1:05:16<09:05,  2.10s/it]



































































































 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 1999/2160 [1:08:49<05:39,  2.11s/it]
 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 2000/2160 [1:08:51<05:36,  2.10s/it][INFO|trainer.py:3081] 2023-11-27 19:37:42,027 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-11-27 19:37:42,028 >>   Num examples = 24
[INFO|trainer.py:3086] 2023-11-27 19:37:42,028 >>   Batch size = 1
 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 4/6 [00:00<00:00, 14.22it/s]
{'eval_loss': 0.6314184665679932, 'eval_runtime': 1.2551, 'eval_samples_per_second': 19.122, 'eval_steps_per_second': 4.78, 'epoch': 27.78}
11/27/2023 19:37:43 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2000
[INFO|tokenization_utils_base.py:2217] 2023-11-27 19:37:43,514 >> Special tokens file saved in sft_model_path/checkpoint-2000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-11-27 19:37:43,949 >> Deleting older checkpoint [sft_model_path/checkpoint-1500] due to args.save_total_limit

































































































 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌    | 2099/2160 [1:12:26<02:10,  2.14s/it]


























































100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2160/2160 [1:14:27<00:00,  1.94s/it][INFO|trainer.py:1934] 2023-11-27 19:43:17,454 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2160/2160 [1:14:27<00:00,  2.07s/it]
[INFO|tokenization_utils_base.py:2210] 2023-11-27 19:43:17,823 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-11-27 19:43:17,824 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-11-27 19:43:17,833 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-11-27 19:43:17,833 >>   Num examples = 24
[INFO|trainer.py:3086] 2023-11-27 19:43:17,833 >>   Batch size = 1
{'train_runtime': 4479.4576, 'train_samples_per_second': 15.43, 'train_steps_per_second': 0.482, 'train_loss': 0.6198613516710423, 'epoch': 30.0}
11/27/2023 19:43:17 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
11/27/2023 19:43:17 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =       30.0
  train_loss               =     0.6199
  train_runtime            = 1:14:39.45
  train_samples_per_second =      15.43
  train_steps_per_second   =      0.482
***** eval metrics *****
  epoch                   =       30.0
  eval_loss               =     0.6321
  eval_runtime            = 0:00:01.04
  eval_samples_per_second =     22.959
  eval_steps_per_second   =       5.74
  perplexity              =     1.8815
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 12.20it/s]