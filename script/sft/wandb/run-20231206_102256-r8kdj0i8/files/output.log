

  0%|                                                                                                                                                                 | 1/3770 [00:03<3:47:26,  3.62s/it]




























































































  3%|████▏                                                                                                                                                          | 100/3770 [03:07<1:52:38,  1.84s/it]





























































































  5%|████████▍                                                                                                                                                      | 200/3770 [06:14<1:53:49,  1.91s/it]






























































































  8%|████████████▋                                                                                                                                                  | 300/3770 [09:22<1:50:04,  1.90s/it]





























































































 11%|████████████████▊                                                                                                                                              | 400/3770 [12:31<1:46:05,  1.89s/it]



























































































 13%|█████████████████████                                                                                                                                          | 499/3770 [15:35<1:41:46,  1.87s/it]
 13%|█████████████████████                                                                                                                                          | 500/3770 [15:37<1:41:10,  1.86s/it][INFO|trainer.py:3081] 2023-12-06 10:38:42,432 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 10:38:42,432 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 10:38:42,432 >>   Batch size = 1

 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋     | 30/31 [00:02<00:00, 13.67it/s]
{'eval_loss': 0.5006787180900574, 'eval_runtime': 3.3318, 'eval_samples_per_second': 18.309, 'eval_steps_per_second': 9.304, 'epoch': 1.32}
12/06/2023 10:38:45 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-500
[INFO|tokenization_utils_base.py:2217] 2023-12-06 10:38:45,991 >> Special tokens file saved in sft_model_path/checkpoint-500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-06 10:38:46,414 >> Deleting older checkpoint [sft_model_path/checkpoint-6000] due to args.save_total_limit


























































































 16%|█████████████████████████▎                                                                                                                                     | 599/3770 [18:43<1:38:48,  1.87s/it]





























































































 19%|█████████████████████████████▍                                                                                                                                 | 699/3770 [21:49<1:35:22,  1.86s/it]




























































































 21%|█████████████████████████████████▋                                                                                                                             | 799/3770 [24:54<1:31:10,  1.84s/it]





























































































 24%|█████████████████████████████████████▉                                                                                                                         | 899/3770 [28:01<1:30:54,  1.90s/it]





























































































 27%|█████████████████████████████████████████▉                                                                                                                    | 1000/3770 [31:10<1:25:56,  1.86s/it][INFO|trainer.py:3081] 2023-12-06 10:54:15,849 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 10:54:15,849 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 10:54:15,849 >>   Batch size = 1
{'loss': 0.4296, 'learning_rate': 8.619568527465547e-06, 'epoch': 2.65}
 61%|████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                               | 19/31 [00:01<00:00, 14.13it/s]
{'eval_loss': 0.4214789867401123, 'eval_runtime': 3.2426, 'eval_samples_per_second': 18.812, 'eval_steps_per_second': 9.56, 'epoch': 2.65}
12/06/2023 10:54:19 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1000
[INFO|tokenization_utils_base.py:2217] 2023-12-06 10:54:19,321 >> Special tokens file saved in sft_model_path/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-06 10:54:19,745 >> Deleting older checkpoint [sft_model_path/checkpoint-500] due to args.save_total_limit


























































































 29%|██████████████████████████████████████████████                                                                                                                | 1099/3770 [34:15<1:20:55,  1.82s/it]




























































































 32%|██████████████████████████████████████████████████▎                                                                                                           | 1200/3770 [37:22<1:17:19,  1.81s/it]



























































































 34%|██████████████████████████████████████████████████████▍                                                                                                       | 1300/3770 [40:24<1:14:54,  1.82s/it]



























































































 37%|██████████████████████████████████████████████████████████▋                                                                                                   | 1400/3770 [43:27<1:12:19,  1.83s/it]


























































































 40%|██████████████████████████████████████████████████████████████▊                                                                                               | 1499/3770 [46:27<1:08:01,  1.80s/it]
 40%|██████████████████████████████████████████████████████████████▊                                                                                               | 1500/3770 [46:29<1:07:58,  1.80s/it][INFO|trainer.py:3081] 2023-12-06 11:09:34,215 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 11:09:34,215 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 11:09:34,215 >>   Batch size = 1
 48%|███████████████████████████████████████████████████████████████████████████████▎                                                                                    | 15/31 [00:01<00:01, 14.37it/s]
{'eval_loss': 0.38674721121788025, 'eval_runtime': 3.277, 'eval_samples_per_second': 18.615, 'eval_steps_per_second': 9.46, 'epoch': 3.97}
12/06/2023 11:09:37 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1500
[INFO|tokenization_utils_base.py:2217] 2023-12-06 11:09:37,718 >> Special tokens file saved in sft_model_path/checkpoint-1500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-06 11:09:38,135 >> Deleting older checkpoint [sft_model_path/checkpoint-1000] due to args.save_total_limit



























































































 42%|███████████████████████████████████████████████████████████████████                                                                                           | 1599/3770 [49:36<1:06:37,  1.84s/it]




























































































 45%|███████████████████████████████████████████████████████████████████████▏                                                                                      | 1699/3770 [52:40<1:03:28,  1.84s/it]




























































































 48%|████████████████████████████████████████████████████████████████████████████▎                                                                                   | 1799/3770 [55:45<59:58,  1.83s/it]


























































































 50%|████████████████████████████████████████████████████████████████████████████████▌                                                                               | 1899/3770 [58:49<57:09,  1.83s/it]



























































































 53%|███████████████████████████████████████████████████████████████████████████████████▊                                                                          | 1999/3770 [1:01:51<53:40,  1.82s/it]
 53%|███████████████████████████████████████████████████████████████████████████████████▊                                                                          | 2000/3770 [1:01:53<53:29,  1.81s/it][INFO|trainer.py:3081] 2023-12-06 11:24:58,610 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 11:24:58,610 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 11:24:58,610 >>   Batch size = 1

[INFO|tokenization_utils_base.py:2217] 2023-12-06 11:25:02,102 >> Special tokens file saved in sft_model_path/checkpoint-2000/special_tokens_map.json
{'eval_loss': 0.3627316653728485, 'eval_runtime': 3.2651, 'eval_samples_per_second': 18.682, 'eval_steps_per_second': 9.494, 'epoch': 5.3}
12/06/2023 11:25:01 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2000
12/06/2023 11:25:01 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2894] 2023-12-06 11:25:02,528 >> Deleting older checkpoint [sft_model_path/checkpoint-1500] due to args.save_total_limit

























































































 56%|███████████████████████████████████████████████████████████████████████████████████████▉                                                                      | 2099/3770 [1:04:58<51:03,  1.83s/it]




























































































 58%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                 | 2199/3770 [1:08:02<47:39,  1.82s/it]




























































































 61%|████████████████████████████████████████████████████████████████████████████████████████████████▍                                                             | 2300/3770 [1:11:08<44:37,  1.82s/it]


























































































 64%|████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 2399/3770 [1:14:08<41:29,  1.82s/it]



























































































 66%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                     | 2500/3770 [1:17:14<38:24,  1.81s/it][INFO|trainer.py:3081] 2023-12-06 11:40:19,995 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 11:40:19,995 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 11:40:19,995 >>   Batch size = 1
{'loss': 0.2771, 'learning_rate': 2.6933744369147265e-06, 'epoch': 6.62}
[INFO|tokenization_utils_base.py:2217] 2023-12-06 11:40:23,607 >> Special tokens file saved in sft_model_path/checkpoint-2500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-06 11:40:24,024 >> Deleting older checkpoint [sft_model_path/checkpoint-2000] due to args.save_total_limit
{'eval_loss': 0.34491071105003357, 'eval_runtime': 3.3868, 'eval_samples_per_second': 18.011, 'eval_steps_per_second': 9.153, 'epoch': 6.62}
12/06/2023 11:40:23 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2500
12/06/2023 11:40:23 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



























































































 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                 | 2599/3770 [1:20:23<35:46,  1.83s/it]



























































































 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                             | 2699/3770 [1:23:26<32:53,  1.84s/it]





























































































 74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 2799/3770 [1:26:32<29:47,  1.84s/it]



























































































 77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                    | 2899/3770 [1:29:35<26:18,  1.81s/it]



























































































 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                | 3000/3770 [1:32:40<24:24,  1.90s/it][INFO|trainer.py:3081] 2023-12-06 11:55:45,935 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 11:55:45,936 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 11:55:45,936 >>   Batch size = 1
{'loss': 0.2584, 'learning_rate': 1.0551309534603876e-06, 'epoch': 7.95}
 61%|████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                               | 19/31 [00:01<00:00, 14.12it/s]
{'eval_loss': 0.3388219475746155, 'eval_runtime': 3.2397, 'eval_samples_per_second': 18.829, 'eval_steps_per_second': 9.569, 'epoch': 7.95}
12/06/2023 11:55:49 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-3000
[INFO|tokenization_utils_base.py:2217] 2023-12-06 11:55:49,401 >> Special tokens file saved in sft_model_path/checkpoint-3000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-06 11:55:49,825 >> Deleting older checkpoint [sft_model_path/checkpoint-2500] due to args.save_total_limit





























































































 82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                            | 3100/3770 [1:35:54<21:17,  1.91s/it]































































































 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                        | 3200/3770 [1:39:05<17:47,  1.87s/it]






























































































 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 3300/3770 [1:42:14<15:10,  1.94s/it]































































































 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍               | 3400/3770 [1:45:27<12:27,  2.02s/it]





























































































 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋           | 3500/3770 [1:48:37<08:21,  1.86s/it][INFO|trainer.py:3081] 2023-12-06 12:11:42,337 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 12:11:42,337 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 12:11:42,337 >>   Batch size = 1
{'loss': 0.2497, 'learning_rate': 1.3396948981624304e-07, 'epoch': 9.27}
[INFO|tokenization_utils_base.py:2217] 2023-12-06 12:11:45,813 >> Special tokens file saved in sft_model_path/checkpoint-3500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-06 12:11:46,237 >> Deleting older checkpoint [sft_model_path/checkpoint-3000] due to args.save_total_limit
{'eval_loss': 0.34034913778305054, 'eval_runtime': 3.2524, 'eval_samples_per_second': 18.755, 'eval_steps_per_second': 9.531, 'epoch': 9.27}
12/06/2023 12:11:45 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-3500
12/06/2023 12:11:45 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



























































































 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊       | 3599/3770 [1:51:44<05:18,  1.86s/it]



























































































 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 3699/3770 [1:54:48<02:08,  1.81s/it]

































































100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 3769/3770 [1:56:57<00:01,  1.85s/it]
{'train_runtime': 7029.7185, 'train_samples_per_second': 8.591, 'train_steps_per_second': 0.536, 'train_loss': 0.46178022088675663, 'epoch': 9.99}
12/06/2023 12:20:04 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
12/06/2023 12:20:04 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =       9.99
  train_loss               =     0.4618
  train_runtime            = 1:57:09.71
  train_samples_per_second =      8.591
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3770/3770 [1:56:58<00:00,  1.85s/it][INFO|trainer.py:1934] 2023-12-06 12:20:04,133 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3770/3770 [1:56:59<00:00,  1.86s/it]
[INFO|tokenization_utils_base.py:2210] 2023-12-06 12:20:04,452 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-06 12:20:04,452 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-12-06 12:20:04,461 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-06 12:20:04,462 >>   Num examples = 61
[INFO|trainer.py:3086] 2023-12-06 12:20:04,462 >>   Batch size = 1

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 13.41it/s]
***** eval metrics *****
  epoch                   =       9.99
  eval_loss               =     0.3396
  eval_runtime            = 0:00:03.22
  eval_samples_per_second =     18.916
  eval_steps_per_second   =      9.613
  perplexity              =     1.4043