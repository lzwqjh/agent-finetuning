

  0%|                                                                                                                                                                 | 1/6090 [00:04<7:14:47,  4.28s/it]


































































































  2%|██▌                                                                                                                                                             | 99/6090 [03:55<3:51:43,  2.32s/it]




































































































  3%|█████▏                                                                                                                                                         | 199/6090 [07:49<3:48:04,  2.32s/it]





































































































  5%|███████▊                                                                                                                                                       | 300/6090 [11:45<3:43:44,  2.32s/it]



































































































  7%|██████████▍                                                                                                                                                    | 400/6090 [15:35<3:31:09,  2.23s/it]



































































































  8%|█████████████                                                                                                                                                  | 500/6090 [19:18<3:29:36,  2.25s/it][INFO|trainer.py:3081] 2023-12-05 10:45:23,357 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-05 10:45:23,357 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-05 10:45:23,357 >>   Batch size = 1
{'loss': 1.1811, 'learning_rate': 9.929553808336842e-06, 'epoch': 0.82}


[INFO|tokenization_utils_base.py:2217] 2023-12-05 10:45:29,114 >> Special tokens file saved in sft_model_path/checkpoint-500/special_tokens_map.json
{'eval_loss': 1.1096071004867554, 'eval_runtime': 5.509, 'eval_samples_per_second': 17.971, 'eval_steps_per_second': 9.076, 'epoch': 0.82}
12/05/2023 10:45:28 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-500
12/05/2023 10:45:28 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2894] 2023-12-05 10:45:29,581 >> Deleting older checkpoint [sft_model_path/checkpoint-1500] due to args.save_total_limit





























































































 10%|███████████████▋                                                                                                                                               | 600/6090 [23:09<3:22:44,  2.22s/it]




































































































 11%|██████████████████▎                                                                                                                                            | 700/6090 [26:54<3:27:47,  2.31s/it]



































































































 13%|████████████████████▊                                                                                                                                          | 799/6090 [30:35<3:18:10,  2.25s/it]



































































































 15%|███████████████████████▍                                                                                                                                       | 899/6090 [34:19<3:21:00,  2.32s/it]


































































































 16%|█████████████████████████▉                                                                                                                                    | 1000/6090 [38:02<3:15:00,  2.30s/it][INFO|trainer.py:3081] 2023-12-05 11:04:07,758 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-05 11:04:07,758 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-05 11:04:07,758 >>   Batch size = 1
{'loss': 1.0986, 'learning_rate': 9.536490017597762e-06, 'epoch': 1.64}

 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                | 40/50 [00:03<00:00, 11.82it/s]
{'eval_loss': 1.0804762840270996, 'eval_runtime': 5.4252, 'eval_samples_per_second': 18.248, 'eval_steps_per_second': 9.216, 'epoch': 1.64}
12/05/2023 11:04:13 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1000
[INFO|tokenization_utils_base.py:2217] 2023-12-05 11:04:13,431 >> Special tokens file saved in sft_model_path/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-05 11:04:13,889 >> Deleting older checkpoint [sft_model_path/checkpoint-500] due to args.save_total_limit


































































































 18%|████████████████████████████▌                                                                                                                                 | 1099/6090 [41:50<3:05:43,  2.23s/it]




































































































 20%|███████████████████████████████▏                                                                                                                              | 1200/6090 [45:35<3:00:01,  2.21s/it]




































































































 21%|█████████████████████████████████▋                                                                                                                            | 1300/6090 [49:19<2:58:57,  2.24s/it]




































































































 23%|████████████████████████████████████▎                                                                                                                         | 1400/6090 [53:01<2:52:32,  2.21s/it]



































































































 25%|██████████████████████████████████████▉                                                                                                                       | 1499/6090 [56:51<2:54:46,  2.28s/it]
 25%|██████████████████████████████████████▉                                                                                                                       | 1500/6090 [56:53<2:55:23,  2.29s/it][INFO|trainer.py:3081] 2023-12-05 11:22:58,397 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-05 11:22:58,398 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-05 11:22:58,398 >>   Batch size = 1


[INFO|tokenization_utils_base.py:2217] 2023-12-05 11:23:04,088 >> Special tokens file saved in sft_model_path/checkpoint-1500/special_tokens_map.json
{'eval_loss': 1.0696499347686768, 'eval_runtime': 5.4318, 'eval_samples_per_second': 18.226, 'eval_steps_per_second': 9.205, 'epoch': 2.46}
12/05/2023 11:23:03 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1500
12/05/2023 11:23:03 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2894] 2023-12-05 11:23:04,551 >> Deleting older checkpoint [sft_model_path/checkpoint-1000] due to args.save_total_limit



































































































 26%|████████████████████████████████████████▉                                                                                                                   | 1600/6090 [1:00:48<2:49:19,  2.26s/it]


































































































 28%|███████████████████████████████████████████▌                                                                                                                | 1699/6090 [1:04:29<2:42:29,  2.22s/it]




































































































 30%|██████████████████████████████████████████████                                                                                                              | 1799/6090 [1:08:11<2:42:53,  2.28s/it]




































































































 31%|████████████████████████████████████████████████▋                                                                                                           | 1899/6090 [1:11:56<2:36:16,  2.24s/it]




































































































 33%|███████████████████████████████████████████████████▏                                                                                                        | 1999/6090 [1:15:39<2:31:47,  2.23s/it]
 33%|███████████████████████████████████████████████████▏                                                                                                        | 2000/6090 [1:15:41<2:36:13,  2.29s/it][INFO|trainer.py:3081] 2023-12-05 11:41:46,835 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-05 11:41:46,836 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-05 11:41:46,836 >>   Batch size = 1

 66%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                       | 33/50 [00:02<00:01, 12.12it/s]
{'eval_loss': 1.0628176927566528, 'eval_runtime': 5.356, 'eval_samples_per_second': 18.484, 'eval_steps_per_second': 9.335, 'epoch': 3.28}
12/05/2023 11:41:52 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2000

[INFO|tokenization_utils_base.py:2217] 2023-12-05 11:41:52,454 >> Special tokens file saved in sft_model_path/checkpoint-2000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-05 11:41:52,924 >> Deleting older checkpoint [sft_model_path/checkpoint-1500] due to args.save_total_limit



































































































 34%|█████████████████████████████████████████████████████▊                                                                                                      | 2100/6090 [1:19:32<2:27:26,  2.22s/it]



































































































 36%|████████████████████████████████████████████████████████▎                                                                                                   | 2199/6090 [1:23:13<2:25:41,  2.25s/it]




































































































 38%|██████████████████████████████████████████████████████████▉                                                                                                 | 2299/6090 [1:26:57<2:19:43,  2.21s/it]




































































































 39%|█████████████████████████████████████████████████████████████▍                                                                                              | 2399/6090 [1:30:40<2:16:38,  2.22s/it]




































































































 41%|████████████████████████████████████████████████████████████████                                                                                            | 2500/6090 [1:34:27<2:13:54,  2.24s/it][INFO|trainer.py:3081] 2023-12-05 12:00:32,189 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-05 12:00:32,190 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-05 12:00:32,190 >>   Batch size = 1
  4%|██████▌                                                                                                                                                              | 2/50 [00:00<00:03, 13.49it/s]


[INFO|tokenization_utils_base.py:2217] 2023-12-05 12:00:37,917 >> Special tokens file saved in sft_model_path/checkpoint-2500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-05 12:00:38,381 >> Deleting older checkpoint [sft_model_path/checkpoint-2000] due to args.save_total_limit
{'eval_loss': 1.0564848184585571, 'eval_runtime': 5.4692, 'eval_samples_per_second': 18.101, 'eval_steps_per_second': 9.142, 'epoch': 4.1}
12/05/2023 12:00:37 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2500
12/05/2023 12:00:37 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.


































































































 43%|██████████████████████████████████████████████████████████████████▌                                                                                         | 2599/6090 [1:38:14<2:08:20,  2.21s/it]




































































































 44%|█████████████████████████████████████████████████████████████████████▏                                                                                      | 2699/6090 [1:41:57<2:06:05,  2.23s/it]




































































































 46%|███████████████████████████████████████████████████████████████████████▋                                                                                    | 2799/6090 [1:45:41<2:02:07,  2.23s/it]





































































































 48%|██████████████████████████████████████████████████████████████████████████▎                                                                                 | 2900/6090 [1:49:28<1:59:10,  2.24s/it]



































































































 49%|████████████████████████████████████████████████████████████████████████████▊                                                                               | 3000/6090 [1:53:12<1:56:59,  2.27s/it][INFO|trainer.py:3081] 2023-12-05 12:19:17,145 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-05 12:19:17,146 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-05 12:19:17,146 >>   Batch size = 1
{'loss': 1.0592, 'learning_rate': 5.365315796808629e-06, 'epoch': 4.92}

 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉             | 46/50 [00:03<00:00, 11.72it/s]
{'eval_loss': 1.0531649589538574, 'eval_runtime': 5.2942, 'eval_samples_per_second': 18.7, 'eval_steps_per_second': 9.444, 'epoch': 4.92}
12/05/2023 12:19:22 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-3000
[INFO|tokenization_utils_base.py:2217] 2023-12-05 12:19:22,691 >> Special tokens file saved in sft_model_path/checkpoint-3000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-05 12:19:23,150 >> Deleting older checkpoint [sft_model_path/checkpoint-2500] due to args.save_total_limit



































































































 51%|███████████████████████████████████████████████████████████████████████████████▍                                                                            | 3100/6090 [1:57:01<1:49:53,  2.21s/it]




































































































 53%|█████████████████████████████████████████████████████████████████████████████████▉                                                                          | 3200/6090 [2:00:44<1:46:05,  2.20s/it]




































































































 54%|████████████████████████████████████████████████████████████████████████████████████▌                                                                       | 3300/6090 [2:04:27<1:42:53,  2.21s/it]




































































































 56%|███████████████████████████████████████████████████████████████████████████████████████                                                                     | 3400/6090 [2:08:11<1:39:09,  2.21s/it]



































































































 57%|█████████████████████████████████████████████████████████████████████████████████████████▋                                                                  | 3499/6090 [2:11:52<1:35:46,  2.22s/it]
 57%|█████████████████████████████████████████████████████████████████████████████████████████▋                                                                  | 3500/6090 [2:11:54<1:35:31,  2.21s/it][INFO|trainer.py:3081] 2023-12-05 12:37:59,494 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-05 12:37:59,494 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-05 12:37:59,494 >>   Batch size = 1

 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                          | 42/50 [00:03<00:00, 11.82it/s]
{'eval_loss': 1.055315613746643, 'eval_runtime': 5.485, 'eval_samples_per_second': 18.049, 'eval_steps_per_second': 9.116, 'epoch': 5.74}
12/05/2023 12:38:04 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-3500
[INFO|tokenization_utils_base.py:2217] 2023-12-05 12:38:05,238 >> Special tokens file saved in sft_model_path/checkpoint-3500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-05 12:38:05,700 >> Deleting older checkpoint [sft_model_path/checkpoint-3000] due to args.save_total_limit


































































































 59%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                               | 3599/6090 [2:15:42<1:31:46,  2.21s/it]




































































































 61%|██████████████████████████████████████████████████████████████████████████████████████████████▊                                                             | 3699/6090 [2:19:27<1:28:58,  2.23s/it]





































































































 62%|█████████████████████████████████████████████████████████████████████████████████████████████████▎                                                          | 3800/6090 [2:23:12<1:24:22,  2.21s/it]



































































































 64%|███████████████████████████████████████████████████████████████████████████████████████████████████▉                                                        | 3899/6090 [2:26:53<1:19:51,  2.19s/it]




































































































 66%|██████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                     | 4000/6090 [2:30:38<1:17:49,  2.23s/it][INFO|trainer.py:3081] 2023-12-05 12:56:43,311 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-05 12:56:43,312 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-05 12:56:43,312 >>   Batch size = 1
 12%|███████████████████▊                                                                                                                                                 | 6/50 [00:00<00:03, 12.50it/s]


[INFO|tokenization_utils_base.py:2217] 2023-12-05 12:56:48,896 >> Special tokens file saved in sft_model_path/checkpoint-4000/special_tokens_map.json
{'eval_loss': 1.0566147565841675, 'eval_runtime': 5.3311, 'eval_samples_per_second': 18.57, 'eval_steps_per_second': 9.379, 'epoch': 6.57}
12/05/2023 12:56:48 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-4000
12/05/2023 12:56:48 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2894] 2023-12-05 12:56:49,371 >> Deleting older checkpoint [sft_model_path/checkpoint-3500] due to args.save_total_limit



































































































 67%|█████████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 4100/6090 [2:34:28<1:14:01,  2.23s/it]



































































































 69%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                | 4199/6090 [2:38:08<1:09:53,  2.22s/it]





































































































 71%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                             | 4300/6090 [2:41:55<1:07:00,  2.25s/it]



































































































 72%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                           | 4399/6090 [2:45:35<1:02:12,  2.21s/it]




































































































 74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                         | 4499/6090 [2:49:17<57:55,  2.18s/it]
 74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                         | 4500/6090 [2:49:19<58:00,  2.19s/it][INFO|trainer.py:3081] 2023-12-05 13:15:24,366 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-05 13:15:24,366 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-05 13:15:24,366 >>   Batch size = 1

 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 44/50 [00:03<00:00, 11.98it/s]
{'eval_loss': 1.0580824613571167, 'eval_runtime': 5.3208, 'eval_samples_per_second': 18.606, 'eval_steps_per_second': 9.397, 'epoch': 7.39}
12/05/2023 13:15:29 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-4500
[INFO|tokenization_utils_base.py:2217] 2023-12-05 13:15:29,955 >> Special tokens file saved in sft_model_path/checkpoint-4500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-05 13:15:30,426 >> Deleting older checkpoint [sft_model_path/checkpoint-4000] due to args.save_total_limit


































































































 76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                      | 4599/6090 [2:53:05<54:58,  2.21s/it]





































































































 77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                    | 4700/6090 [2:56:48<50:22,  2.17s/it]




































































































 79%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 4800/6090 [3:00:30<46:52,  2.18s/it]




































































































 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                              | 4900/6090 [3:04:13<43:40,  2.20s/it]



































































































 82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                            | 5000/6090 [3:07:54<40:02,  2.20s/it][INFO|trainer.py:3081] 2023-12-05 13:33:59,514 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-05 13:33:59,515 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-05 13:33:59,515 >>   Batch size = 1
{'loss': 1.0299, 'learning_rate': 8.18344455538938e-07, 'epoch': 8.21}


[INFO|tokenization_utils_base.py:2217] 2023-12-05 13:34:05,244 >> Special tokens file saved in sft_model_path/checkpoint-5000/special_tokens_map.json
{'eval_loss': 1.0592604875564575, 'eval_runtime': 5.4685, 'eval_samples_per_second': 18.104, 'eval_steps_per_second': 9.143, 'epoch': 8.21}
12/05/2023 13:34:04 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-5000
12/05/2023 13:34:04 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2894] 2023-12-05 13:34:05,722 >> Deleting older checkpoint [sft_model_path/checkpoint-4500] due to args.save_total_limit



































































































 84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                         | 5100/6090 [3:11:43<36:22,  2.20s/it]



































































































 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                       | 5199/6090 [3:15:23<32:05,  2.16s/it]




































































































 87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                    | 5299/6090 [3:19:04<29:37,  2.25s/it]




































































































 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                  | 5399/6090 [3:22:46<25:00,  2.17s/it]




































































































 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋               | 5499/6090 [3:26:31<22:14,  2.26s/it]
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋               | 5500/6090 [3:26:34<22:39,  2.30s/it][INFO|trainer.py:3081] 2023-12-05 13:52:39,229 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-05 13:52:39,229 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-05 13:52:39,229 >>   Batch size = 1

 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 44/50 [00:03<00:00, 11.41it/s]
{'eval_loss': 1.0594515800476074, 'eval_runtime': 5.4396, 'eval_samples_per_second': 18.2, 'eval_steps_per_second': 9.192, 'epoch': 9.03}
12/05/2023 13:52:44 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-5500
[INFO|tokenization_utils_base.py:2217] 2023-12-05 13:52:44,922 >> Special tokens file saved in sft_model_path/checkpoint-5500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-05 13:52:45,396 >> Deleting older checkpoint [sft_model_path/checkpoint-5000] due to args.save_total_limit


































































































 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎            | 5599/6090 [3:30:19<18:04,  2.21s/it]




































































































 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊          | 5699/6090 [3:34:00<14:23,  2.21s/it]





































































































 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍       | 5800/6090 [3:37:43<10:40,  2.21s/it]



































































































 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████     | 5899/6090 [3:41:22<06:58,  2.19s/it]



































































































 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 6000/6090 [3:45:05<03:18,  2.21s/it][INFO|trainer.py:3081] 2023-12-05 14:11:10,142 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-05 14:11:10,143 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-05 14:11:10,143 >>   Batch size = 1
{'loss': 1.0114, 'learning_rate': 5.854688907053207e-09, 'epoch': 9.85}

[INFO|tokenization_utils_base.py:2217] 2023-12-05 14:11:15,763 >> Special tokens file saved in sft_model_path/checkpoint-6000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-05 14:11:16,230 >> Deleting older checkpoint [sft_model_path/checkpoint-5500] due to args.save_total_limit
{'eval_loss': 1.0596767663955688, 'eval_runtime': 5.3736, 'eval_samples_per_second': 18.424, 'eval_steps_per_second': 9.305, 'epoch': 9.85}
12/05/2023 14:11:15 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-6000
12/05/2023 14:11:15 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
























































































100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 6089/6090 [3:48:28<00:02,  2.25s/it]
{'train_runtime': 13720.6389, 'train_samples_per_second': 7.104, 'train_steps_per_second': 0.444, 'train_loss': 1.085954866933901, 'epoch': 10.0}
12/05/2023 14:14:35 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
12/05/2023 14:14:35 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =       10.0
  train_loss               =      1.086
  train_runtime            = 3:48:40.63
  train_samples_per_second =      7.104
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6090/6090 [3:48:30<00:00,  2.24s/it][INFO|trainer.py:1934] 2023-12-05 14:14:35,585 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6090/6090 [3:48:30<00:00,  2.25s/it]
[INFO|tokenization_utils_base.py:2210] 2023-12-05 14:14:35,964 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-05 14:14:35,964 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-12-05 14:14:35,976 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-05 14:14:35,976 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-05 14:14:35,976 >>   Batch size = 1


 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                | 45/50 [00:03<00:00, 11.86it/s]
***** eval metrics *****
  epoch                   =       10.0
  eval_loss               =     1.0597
  eval_runtime            = 0:00:05.18
  eval_samples_per_second =     19.103
  eval_steps_per_second   =      9.648

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.58it/s]