

  0%|▎                                                                                                                                                                   | 1/585 [00:04<40:22,  4.15s/it]



































































































 17%|███████████████████████████▋                                                                                                                                      | 100/585 [03:59<18:56,  2.34s/it]



































































































 34%|███████████████████████████████████████████████████████▍                                                                                                          | 200/585 [08:06<15:02,  2.35s/it]



































































































 51%|██████████████████████████████████████████████████████████████████████████████████▊                                                                               | 299/585 [12:06<11:23,  2.39s/it]



































































































 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                   | 400/585 [16:10<07:18,  2.37s/it]



































































































 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                       | 500/585 [20:13<03:32,  2.50s/it][INFO|trainer.py:3081] 2023-12-07 09:24:26,584 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-07 09:24:26,584 >>   Num examples = 19
[INFO|trainer.py:3086] 2023-12-07 09:24:26,585 >>   Batch size = 1
{'loss': 0.7421, 'learning_rate': 5.443386905823045e-07, 'epoch': 4.27}
 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                       | 500/585 [20:16<03:32,  2.50s/it][INFO|tokenization_utils_base.py:2210] 2023-12-07 09:24:29,103 >> tokenizer config file saved in sft_model_path/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-07 09:24:29,103 >> Special tokens file saved in sft_model_path/checkpoint-500/special_tokens_map.json
{'eval_loss': 0.7534961700439453, 'eval_runtime': 2.2171, 'eval_samples_per_second': 8.57, 'eval_steps_per_second': 4.51, 'epoch': 4.27}
12/07/2023 09:24:28 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-500
12/07/2023 09:24:28 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



















































































100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 584/585 [23:40<00:02,  2.40s/it]
{'train_runtime': 1434.0113, 'train_samples_per_second': 6.527, 'train_steps_per_second': 0.408, 'train_loss': 1.2813087683457596, 'epoch': 5.0}
12/07/2023 09:27:55 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
12/07/2023 09:27:55 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     1.2813
  train_runtime            = 0:23:54.01
  train_samples_per_second =      6.527
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 585/585 [23:43<00:00,  2.44s/it][INFO|trainer.py:1934] 2023-12-07 09:27:55,738 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 585/585 [23:43<00:00,  2.43s/it]
[INFO|tokenization_utils_base.py:2210] 2023-12-07 09:27:56,118 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-07 09:27:56,118 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-12-07 09:27:56,128 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-07 09:27:56,128 >>   Num examples = 19
[INFO|trainer.py:3086] 2023-12-07 09:27:56,128 >>   Batch size = 1
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.68it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_loss               =     0.7511
  eval_runtime            = 0:00:02.05
  eval_samples_per_second =      9.249
  eval_steps_per_second   =      4.868
  perplexity              =     2.1194