

  0%|▎                                                                                                                                            | 1/450 [00:06<47:13,  6.31s/it]


































































































 22%|██████████████████████████████▊                                                                                                             | 99/450 [05:42<19:50,  3.39s/it]




































































































 44%|█████████████████████████████████████████████████████████████▊                                                                             | 200/450 [11:02<12:59,  3.12s/it][INFO|trainer.py:3081] 2023-10-22 16:13:46,353 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-22 16:13:46,353 >>   Num examples = 3
[INFO|trainer.py:3086] 2023-10-22 16:13:46,353 >>   Batch size = 1
{'loss': 0.7582, 'learning_rate': 6.142689677583447e-06, 'epoch': 12.9}
{'eval_loss': 0.553088366985321, 'eval_runtime': 1.3847, 'eval_samples_per_second': 2.167, 'eval_steps_per_second': 1.444, 'epoch': 12.9}
10/22/2023 16:13:47 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-200
10/22/2023 16:13:47 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
 44%|█████████████████████████████████████████████████████████████▊                                                                             | 200/450 [11:04<12:59,  3.12s/it][INFO|tokenization_utils_base.py:2210] 2023-10-22 16:13:47,998 >> tokenizer config file saved in sft_model_path/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-10-22 16:13:47,998 >> Special tokens file saved in sft_model_path/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-22 16:13:48,496 >> Deleting older checkpoint [sft_model_path/checkpoint-25500] due to args.save_total_limit


































































































 66%|████████████████████████████████████████████████████████████████████████████████████████████▎                                              | 299/450 [16:19<08:06,  3.22s/it]




































































































 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌               | 400/450 [21:37<02:31,  3.03s/it][INFO|trainer.py:3081] 2023-10-22 16:24:20,994 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-22 16:24:20,994 >>   Num examples = 3
[INFO|trainer.py:3086] 2023-10-22 16:24:20,995 >>   Batch size = 1
  0%|                                                                                                                                                       | 0/2 [00:00<?, ?it/s]
{'loss': 0.4973, 'learning_rate': 3.20999354897229e-07, 'epoch': 25.81}
{'eval_loss': 0.4931998550891876, 'eval_runtime': 1.4232, 'eval_samples_per_second': 2.108, 'eval_steps_per_second': 1.405, 'epoch': 25.81}
10/22/2023 16:24:22 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-400
[INFO|tokenization_utils_base.py:2217] 2023-10-22 16:24:22,684 >> Special tokens file saved in sft_model_path/checkpoint-400/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-22 16:24:23,164 >> Deleting older checkpoint [sft_model_path/checkpoint-200] due to args.save_total_limit
















































100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 450/450 [24:19<00:00,  3.56s/it][INFO|trainer.py:1934] 2023-10-22 16:27:03,340 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 450/450 [24:19<00:00,  3.24s/it]
[INFO|tokenization_utils_base.py:2210] 2023-10-22 16:27:03,712 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-10-22 16:27:03,712 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-10-22 16:27:03,721 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-22 16:27:03,721 >>   Num examples = 3
[INFO|trainer.py:3086] 2023-10-22 16:27:03,722 >>   Batch size = 1
{'train_runtime': 1470.3512, 'train_samples_per_second': 5.06, 'train_steps_per_second': 0.306, 'train_loss': 0.88100235303243, 'epoch': 29.03}
10/22/2023 16:27:03 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
10/22/2023 16:27:03 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =      29.03
  train_loss               =      0.881
  train_runtime            = 0:24:30.35
  train_samples_per_second =       5.06
  train_steps_per_second   =      0.306
***** eval metrics *****
  epoch                   =      29.03
  eval_loss               =     0.4927
  eval_runtime            = 0:00:01.28
  eval_samples_per_second =      2.337
  eval_steps_per_second   =      1.558
  perplexity              =     1.6367
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.04it/s]