

  0%|                                                                                                                                                                | 1/9555 [00:04<12:51:38,  4.85s/it]



































































































  1%|█▋                                                                                                                                                             | 100/9555 [04:16<6:48:01,  2.59s/it]



































































































  2%|███▎                                                                                                                                                           | 199/9555 [08:26<6:36:34,  2.54s/it]




































































































  3%|████▉                                                                                                                                                          | 299/9555 [12:36<6:27:42,  2.51s/it]





































































































  4%|██████▋                                                                                                                                                        | 400/9555 [16:49<6:20:59,  2.50s/it]



































































































  5%|████████▎                                                                                                                                                      | 499/9555 [21:01<6:32:54,  2.60s/it]
  5%|████████▎                                                                                                                                                      | 500/9555 [21:03<6:26:10,  2.56s/it][INFO|trainer.py:3081] 2023-12-07 19:41:33,153 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-07 19:41:33,154 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-07 19:41:33,154 >>   Batch size = 1








[INFO|tokenization_utils_base.py:2217] 2023-12-07 19:41:52,285 >> Special tokens file saved in sft_model_path/checkpoint-500/special_tokens_map.json
{'eval_loss': 0.6064307689666748, 'eval_runtime': 18.648, 'eval_samples_per_second': 16.57, 'eval_steps_per_second': 8.312, 'epoch': 0.26}
12/07/2023 19:41:51 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-500
12/07/2023 19:41:51 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



































































































  6%|█████████▉                                                                                                                                                     | 600/9555 [25:36<6:06:22,  2.45s/it]




































































































  7%|███████████▋                                                                                                                                                   | 700/9555 [29:48<6:18:19,  2.56s/it]



































































































  8%|█████████████▎                                                                                                                                                 | 799/9555 [33:58<5:59:21,  2.46s/it]





































































































  9%|██████████████▉                                                                                                                                                | 900/9555 [38:08<5:47:26,  2.41s/it]



































































































 10%|████████████████▌                                                                                                                                              | 999/9555 [42:10<5:53:04,  2.48s/it]
 10%|████████████████▌                                                                                                                                             | 1000/9555 [42:12<5:50:17,  2.46s/it][INFO|trainer.py:3081] 2023-12-07 20:02:42,180 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-07 20:02:42,180 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-07 20:02:42,180 >>   Batch size = 1






 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 144/155 [00:13<00:00, 11.24it/s]
{'eval_loss': 0.3750208914279938, 'eval_runtime': 16.0993, 'eval_samples_per_second': 19.193, 'eval_steps_per_second': 9.628, 'epoch': 0.52}
12/07/2023 20:02:58 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1000
[INFO|tokenization_utils_base.py:2217] 2023-12-07 20:02:58,544 >> Special tokens file saved in sft_model_path/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-07 20:02:59,027 >> Deleting older checkpoint [sft_model_path/checkpoint-500] due to args.save_total_limit


































































































 12%|██████████████████▏                                                                                                                                           | 1099/9555 [46:30<5:39:27,  2.41s/it]




































































































 13%|███████████████████▊                                                                                                                                          | 1199/9555 [50:35<5:57:37,  2.57s/it]




































































































 14%|█████████████████████▍                                                                                                                                        | 1299/9555 [54:37<5:34:13,  2.43s/it]





































































































 15%|███████████████████████▏                                                                                                                                      | 1400/9555 [58:43<5:40:58,  2.51s/it]



































































































 16%|████████████████████████▍                                                                                                                                   | 1500/9555 [1:02:50<5:20:48,  2.39s/it][INFO|trainer.py:3081] 2023-12-07 20:23:19,810 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-07 20:23:19,810 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-07 20:23:19,810 >>   Batch size = 1
{'loss': 0.3845, 'learning_rate': 9.583263003016414e-06, 'epoch': 0.78}







 87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 135/155 [00:12<00:01, 11.36it/s]
{'eval_loss': 0.30509448051452637, 'eval_runtime': 16.2125, 'eval_samples_per_second': 19.059, 'eval_steps_per_second': 9.561, 'epoch': 0.78}
12/07/2023 20:23:36 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1500
 16%|████████████████████████▍                                                                                                                                   | 1500/9555 [1:03:06<5:20:48,  2.39s/it][INFO|tokenization_utils_base.py:2210] 2023-12-07 20:23:36,273 >> tokenizer config file saved in sft_model_path/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-07 20:23:36,273 >> Special tokens file saved in sft_model_path/checkpoint-1500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-07 20:23:36,745 >> Deleting older checkpoint [sft_model_path/checkpoint-1000] due to args.save_total_limit


































































































 17%|██████████████████████████                                                                                                                                  | 1599/9555 [1:07:08<5:15:02,  2.38s/it]





































































































 18%|███████████████████████████▊                                                                                                                                | 1700/9555 [1:11:14<5:17:33,  2.43s/it]



































































































 19%|█████████████████████████████▎                                                                                                                              | 1799/9555 [1:15:18<5:10:45,  2.40s/it]




































































































 20%|███████████████████████████████                                                                                                                             | 1899/9555 [1:19:23<5:04:29,  2.39s/it]




































































































 21%|████████████████████████████████▋                                                                                                                           | 1999/9555 [1:23:29<5:07:40,  2.44s/it]
 21%|████████████████████████████████▋                                                                                                                           | 2000/9555 [1:23:31<5:08:35,  2.45s/it][INFO|trainer.py:3081] 2023-12-07 20:44:01,378 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-07 20:44:01,378 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-07 20:44:01,378 >>   Batch size = 1







 95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋        | 147/155 [00:13<00:00, 11.14it/s]
{'eval_loss': 0.2830152213573456, 'eval_runtime': 16.1588, 'eval_samples_per_second': 19.123, 'eval_steps_per_second': 9.592, 'epoch': 1.05}
12/07/2023 20:44:17 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2000
[INFO|tokenization_utils_base.py:2217] 2023-12-07 20:44:17,803 >> Special tokens file saved in sft_model_path/checkpoint-2000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-07 20:44:18,288 >> Deleting older checkpoint [sft_model_path/checkpoint-1500] due to args.save_total_limit


































































































 22%|██████████████████████████████████▎                                                                                                                         | 2099/9555 [1:27:47<4:59:27,  2.41s/it]




































































































 23%|███████████████████████████████████▉                                                                                                                        | 2199/9555 [1:31:50<4:59:14,  2.44s/it]





































































































 24%|█████████████████████████████████████▌                                                                                                                      | 2300/9555 [1:35:57<5:07:17,  2.54s/it]




































































































 25%|███████████████████████████████████████▏                                                                                                                    | 2400/9555 [1:40:02<4:41:10,  2.36s/it]



































































































 26%|████████████████████████████████████████▊                                                                                                                   | 2500/9555 [1:44:09<5:05:36,  2.60s/it][INFO|trainer.py:3081] 2023-12-07 21:04:39,056 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-07 21:04:39,057 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-07 21:04:39,057 >>   Batch size = 1
{'loss': 0.3197, 'learning_rate': 8.657949866829222e-06, 'epoch': 1.31}







 26%|████████████████████████████████████████▊                                                                                                                   | 2500/9555 [1:44:26<5:05:36,  2.60s/it][INFO|tokenization_utils_base.py:2210] 2023-12-07 21:04:55,940 >> tokenizer config file saved in sft_model_path/checkpoint-2500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-07 21:04:55,941 >> Special tokens file saved in sft_model_path/checkpoint-2500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-07 21:04:56,431 >> Deleting older checkpoint [sft_model_path/checkpoint-2000] due to args.save_total_limit
{'eval_loss': 0.2726976275444031, 'eval_runtime': 16.614, 'eval_samples_per_second': 18.599, 'eval_steps_per_second': 9.329, 'epoch': 1.31}
12/07/2023 21:04:55 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-2500
12/07/2023 21:04:55 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.


































































































 27%|██████████████████████████████████████████▍                                                                                                                 | 2599/9555 [1:48:30<4:42:05,  2.43s/it]




































































































 28%|████████████████████████████████████████████                                                                                                                | 2699/9555 [1:52:33<4:41:26,  2.46s/it]




































































































 29%|█████████████████████████████████████████████▋                                                                                                              | 2799/9555 [1:56:37<4:44:08,  2.52s/it]





































































































 30%|███████████████████████████████████████████████▎                                                                                                            | 2900/9555 [2:00:45<4:44:50,  2.57s/it]



































































































 31%|████████████████████████████████████████████████▉                                                                                                           | 2999/9555 [2:04:46<4:26:40,  2.44s/it]
 31%|████████████████████████████████████████████████▉                                                                                                           | 3000/9555 [2:04:49<4:24:09,  2.42s/it][INFO|trainer.py:3081] 2023-12-07 21:25:18,912 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-07 21:25:18,912 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-07 21:25:18,912 >>   Batch size = 1






 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌         | 146/155 [00:13<00:00, 11.02it/s]
{'eval_loss': 0.26465654373168945, 'eval_runtime': 16.1395, 'eval_samples_per_second': 19.146, 'eval_steps_per_second': 9.604, 'epoch': 1.57}
12/07/2023 21:25:35 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-3000
[INFO|tokenization_utils_base.py:2217] 2023-12-07 21:25:35,313 >> Special tokens file saved in sft_model_path/checkpoint-3000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-07 21:25:35,791 >> Deleting older checkpoint [sft_model_path/checkpoint-2500] due to args.save_total_limit



































































































 32%|██████████████████████████████████████████████████▌                                                                                                         | 3100/9555 [2:09:07<4:20:40,  2.42s/it]




































































































 33%|████████████████████████████████████████████████████▏                                                                                                       | 3200/9555 [2:13:14<4:12:53,  2.39s/it]




































































































 35%|█████████████████████████████████████████████████████▉                                                                                                      | 3300/9555 [2:17:21<4:19:24,  2.49s/it]




































































































 36%|███████████████████████████████████████████████████████▌                                                                                                    | 3400/9555 [2:21:30<4:09:06,  2.43s/it]



































































































 37%|█████████████████████████████████████████████████████████▏                                                                                                  | 3500/9555 [2:25:39<4:11:08,  2.49s/it][INFO|trainer.py:3081] 2023-12-07 21:46:08,791 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-07 21:46:08,792 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-07 21:46:08,792 >>   Batch size = 1
{'loss': 0.3092, 'learning_rate': 7.317841871416899e-06, 'epoch': 1.83}







 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 153/155 [00:14<00:00, 10.50it/s]
{'eval_loss': 0.2592153549194336, 'eval_runtime': 16.4323, 'eval_samples_per_second': 18.804, 'eval_steps_per_second': 9.433, 'epoch': 1.83}
12/07/2023 21:46:25 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-3500
[INFO|tokenization_utils_base.py:2217] 2023-12-07 21:46:25,494 >> Special tokens file saved in sft_model_path/checkpoint-3500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-07 21:46:25,989 >> Deleting older checkpoint [sft_model_path/checkpoint-3000] due to args.save_total_limit


































































































 38%|██████████████████████████████████████████████████████████▊                                                                                                 | 3599/9555 [2:30:02<4:00:20,  2.42s/it]




































































































 39%|████████████████████████████████████████████████████████████▍                                                                                               | 3699/9555 [2:34:12<3:55:42,  2.42s/it]




































































































 40%|██████████████████████████████████████████████████████████████                                                                                              | 3799/9555 [2:38:21<3:49:15,  2.39s/it]




































































































 41%|███████████████████████████████████████████████████████████████▋                                                                                            | 3899/9555 [2:42:27<3:56:18,  2.51s/it]




































































































 42%|█████████████████████████████████████████████████████████████████▎                                                                                          | 4000/9555 [2:46:35<3:47:21,  2.46s/it][INFO|trainer.py:3081] 2023-12-07 22:07:05,514 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-07 22:07:05,514 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-07 22:07:05,514 >>   Batch size = 1
{'loss': 0.3026, 'learning_rate': 6.537345773009558e-06, 'epoch': 2.09}







 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 154/155 [00:14<00:00, 11.01it/s]
{'eval_loss': 0.25259488821029663, 'eval_runtime': 16.4452, 'eval_samples_per_second': 18.79, 'eval_steps_per_second': 9.425, 'epoch': 2.09}
12/07/2023 22:07:21 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-4000
[INFO|tokenization_utils_base.py:2217] 2023-12-07 22:07:22,225 >> Special tokens file saved in sft_model_path/checkpoint-4000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-07 22:07:22,721 >> Deleting older checkpoint [sft_model_path/checkpoint-3500] due to args.save_total_limit


































































































 43%|██████████████████████████████████████████████████████████████████▉                                                                                         | 4099/9555 [2:50:56<3:40:06,  2.42s/it]





































































































 44%|████████████████████████████████████████████████████████████████████▌                                                                                       | 4200/9555 [2:55:01<3:44:22,  2.51s/it]



































































































 45%|██████████████████████████████████████████████████████████████████████▏                                                                                     | 4299/9555 [2:59:07<3:47:55,  2.60s/it]





































































































 46%|███████████████████████████████████████████████████████████████████████▊                                                                                    | 4400/9555 [3:03:20<3:35:41,  2.51s/it]



































































































 47%|█████████████████████████████████████████████████████████████████████████▍                                                                                  | 4500/9555 [3:07:30<3:25:11,  2.44s/it][INFO|trainer.py:3081] 2023-12-07 22:28:00,493 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-07 22:28:00,493 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-07 22:28:00,493 >>   Batch size = 1
{'loss': 0.29, 'learning_rate': 5.7127942488971596e-06, 'epoch': 2.35}







 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋      | 149/155 [00:13<00:00, 10.92it/s]
{'eval_loss': 0.250213086605072, 'eval_runtime': 16.4134, 'eval_samples_per_second': 18.826, 'eval_steps_per_second': 9.443, 'epoch': 2.35}
12/07/2023 22:28:16 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-4500
[INFO|tokenization_utils_base.py:2217] 2023-12-07 22:28:17,164 >> Special tokens file saved in sft_model_path/checkpoint-4500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-07 22:28:17,676 >> Deleting older checkpoint [sft_model_path/checkpoint-4000] due to args.save_total_limit


































































































 48%|███████████████████████████████████████████████████████████████████████████                                                                                 | 4599/9555 [3:11:52<3:21:20,  2.44s/it]





































































































 49%|████████████████████████████████████████████████████████████████████████████▋                                                                               | 4700/9555 [3:16:05<3:22:22,  2.50s/it]




































































































 50%|██████████████████████████████████████████████████████████████████████████████▎                                                                             | 4800/9555 [3:20:11<3:15:37,  2.47s/it]




































































































 51%|████████████████████████████████████████████████████████████████████████████████                                                                            | 4900/9555 [3:24:16<3:11:38,  2.47s/it]



































































































 52%|█████████████████████████████████████████████████████████████████████████████████▋                                                                          | 5000/9555 [3:28:22<3:05:16,  2.44s/it][INFO|trainer.py:3081] 2023-12-07 22:48:51,664 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-07 22:48:51,665 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-07 22:48:51,665 >>   Batch size = 1
{'loss': 0.2809, 'learning_rate': 4.867816315432617e-06, 'epoch': 2.62}







 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                 | 138/155 [00:13<00:01, 11.06it/s]
{'eval_loss': 0.24679696559906006, 'eval_runtime': 17.4256, 'eval_samples_per_second': 17.733, 'eval_steps_per_second': 8.895, 'epoch': 2.62}
12/07/2023 22:49:09 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-5000
 52%|█████████████████████████████████████████████████████████████████████████████████▋                                                                          | 5000/9555 [3:28:39<3:05:16,  2.44s/it][INFO|tokenization_utils_base.py:2210] 2023-12-07 22:49:09,356 >> tokenizer config file saved in sft_model_path/checkpoint-5000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-07 22:49:09,356 >> Special tokens file saved in sft_model_path/checkpoint-5000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-07 22:49:09,849 >> Deleting older checkpoint [sft_model_path/checkpoint-4500] due to args.save_total_limit


































































































 53%|███████████████████████████████████████████████████████████████████████████████████▏                                                                        | 5099/9555 [3:32:44<3:13:28,  2.61s/it]




































































































 54%|████████████████████████████████████████████████████████████████████████████████████▉                                                                       | 5199/9555 [3:36:55<2:59:26,  2.47s/it]




































































































 55%|██████████████████████████████████████████████████████████████████████████████████████▌                                                                     | 5299/9555 [3:41:04<2:55:56,  2.48s/it]





































































































 57%|████████████████████████████████████████████████████████████████████████████████████████▏                                                                   | 5400/9555 [3:45:19<2:51:17,  2.47s/it]



































































































 58%|█████████████████████████████████████████████████████████████████████████████████████████▊                                                                  | 5499/9555 [3:49:23<2:46:35,  2.46s/it]
 58%|█████████████████████████████████████████████████████████████████████████████████████████▊                                                                  | 5500/9555 [3:49:26<2:46:30,  2.46s/it][INFO|trainer.py:3081] 2023-12-07 23:09:55,884 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-07 23:09:55,884 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-07 23:09:55,884 >>   Batch size = 1







 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 151/155 [00:14<00:00, 11.26it/s]
{'eval_loss': 0.24449552595615387, 'eval_runtime': 16.5347, 'eval_samples_per_second': 18.688, 'eval_steps_per_second': 9.374, 'epoch': 2.88}
12/07/2023 23:10:12 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-5500
[INFO|tokenization_utils_base.py:2217] 2023-12-07 23:10:12,690 >> Special tokens file saved in sft_model_path/checkpoint-5500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-07 23:10:13,187 >> Deleting older checkpoint [sft_model_path/checkpoint-5000] due to args.save_total_limit



































































































 59%|███████████████████████████████████████████████████████████████████████████████████████████▍                                                                | 5600/9555 [3:53:53<2:42:06,  2.46s/it]



































































































 60%|█████████████████████████████████████████████████████████████████████████████████████████████                                                               | 5699/9555 [3:57:56<2:35:24,  2.42s/it]





































































































 61%|██████████████████████████████████████████████████████████████████████████████████████████████▋                                                             | 5800/9555 [4:02:08<2:33:36,  2.45s/it]



































































































 62%|████████████████████████████████████████████████████████████████████████████████████████████████▎                                                           | 5899/9555 [4:06:16<2:35:24,  2.55s/it]





































































































 63%|█████████████████████████████████████████████████████████████████████████████████████████████████▉                                                          | 6000/9555 [4:10:24<2:23:56,  2.43s/it]
 63%|█████████████████████████████████████████████████████████████████████████████████████████████████▉                                                          | 6000/9555 [4:10:24<2:23:56,  2.43s/it][INFO|trainer.py:3081] 2023-12-07 23:30:54,277 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-07 23:30:54,277 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-07 23:30:54,278 >>   Batch size = 1







 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 150/155 [00:14<00:00, 11.06it/s]
{'eval_loss': 0.24212463200092316, 'eval_runtime': 16.6965, 'eval_samples_per_second': 18.507, 'eval_steps_per_second': 9.283, 'epoch': 3.14}
12/07/2023 23:31:10 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-6000
[INFO|tokenization_utils_base.py:2217] 2023-12-07 23:31:11,234 >> Special tokens file saved in sft_model_path/checkpoint-6000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-07 23:31:11,726 >> Deleting older checkpoint [sft_model_path/checkpoint-5500] due to args.save_total_limit


































































































 64%|███████████████████████████████████████████████████████████████████████████████████████████████████▌                                                        | 6099/9555 [4:14:47<2:23:41,  2.49s/it]




































































































 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                      | 6199/9555 [4:18:54<2:22:40,  2.55s/it]





































































































 66%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                     | 6300/9555 [4:23:04<2:14:42,  2.48s/it]




































































































 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                   | 6400/9555 [4:27:11<2:09:57,  2.47s/it]



































































































 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████                                                  | 6499/9555 [4:31:15<2:04:31,  2.44s/it]
 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████                                                  | 6500/9555 [4:31:17<2:03:51,  2.43s/it][INFO|trainer.py:3081] 2023-12-07 23:51:47,395 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-07 23:51:47,395 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-07 23:51:47,395 >>   Batch size = 1







 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 154/155 [00:14<00:00, 11.18it/s]
{'eval_loss': 0.24038046598434448, 'eval_runtime': 16.2699, 'eval_samples_per_second': 18.992, 'eval_steps_per_second': 9.527, 'epoch': 3.4}
12/07/2023 23:52:03 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-6500
[INFO|tokenization_utils_base.py:2217] 2023-12-07 23:52:03,929 >> Special tokens file saved in sft_model_path/checkpoint-6500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-07 23:52:04,416 >> Deleting older checkpoint [sft_model_path/checkpoint-6000] due to args.save_total_limit



































































































 69%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                | 6600/9555 [4:35:39<1:58:55,  2.41s/it]



































































































 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                              | 6699/9555 [4:39:46<1:58:27,  2.49s/it]




































































































 71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████                                             | 6799/9555 [4:44:00<1:53:39,  2.47s/it]




































































































 72%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                           | 6899/9555 [4:48:10<1:48:39,  2.45s/it]




































































































 73%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                         | 6999/9555 [4:52:18<1:46:48,  2.51s/it]
 73%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                         | 7000/9555 [4:52:21<1:45:01,  2.47s/it][INFO|trainer.py:3081] 2023-12-08 00:12:50,792 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-08 00:12:50,792 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-08 00:12:50,792 >>   Batch size = 1







[INFO|tokenization_utils_base.py:2217] 2023-12-08 00:13:07,870 >> Special tokens file saved in sft_model_path/checkpoint-7000/special_tokens_map.json
{'eval_loss': 0.2396070808172226, 'eval_runtime': 16.8118, 'eval_samples_per_second': 18.38, 'eval_steps_per_second': 9.22, 'epoch': 3.66}
12/08/2023 00:13:07 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-7000
12/08/2023 00:13:07 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2894] 2023-12-08 00:13:08,370 >> Deleting older checkpoint [sft_model_path/checkpoint-6500] due to args.save_total_limit


































































































 74%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                        | 7099/9555 [4:56:49<1:40:10,  2.45s/it]



































































































 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                      | 7200/9555 [5:01:04<1:39:03,  2.52s/it]




































































































 76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                    | 7300/9555 [5:05:16<1:34:36,  2.52s/it]




































































































 77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                   | 7400/9555 [5:09:31<1:31:00,  2.53s/it]



































































































 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                 | 7499/9555 [5:13:44<1:25:52,  2.51s/it]
 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                 | 7500/9555 [5:13:47<1:26:18,  2.52s/it][INFO|trainer.py:3081] 2023-12-08 00:34:16,805 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-08 00:34:16,805 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-08 00:34:16,805 >>   Batch size = 1






 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍             | 142/155 [00:13<00:01, 10.44it/s]
{'eval_loss': 0.238181471824646, 'eval_runtime': 17.0875, 'eval_samples_per_second': 18.083, 'eval_steps_per_second': 9.071, 'epoch': 3.92}
12/08/2023 00:34:33 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-7500

[INFO|tokenization_utils_base.py:2217] 2023-12-08 00:34:34,163 >> Special tokens file saved in sft_model_path/checkpoint-7500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-08 00:34:34,652 >> Deleting older checkpoint [sft_model_path/checkpoint-7000] due to args.save_total_limit


































































































 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                | 7599/9555 [5:18:20<1:23:50,  2.57s/it]




































































































 81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                              | 7699/9555 [5:22:36<1:18:35,  2.54s/it]




































































































 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                            | 7799/9555 [5:26:51<1:13:27,  2.51s/it]





































































































 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                           | 7900/9555 [5:31:06<1:10:48,  2.57s/it]



































































































 84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 7999/9555 [5:35:21<1:05:39,  2.53s/it]
 84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 8000/9555 [5:35:23<1:06:38,  2.57s/it][INFO|trainer.py:3081] 2023-12-08 00:55:53,342 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-08 00:55:53,342 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-08 00:55:53,342 >>   Batch size = 1







 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                   | 136/155 [00:14<00:01, 10.55it/s]
{'eval_loss': 0.23843835294246674, 'eval_runtime': 17.8168, 'eval_samples_per_second': 17.343, 'eval_steps_per_second': 8.7, 'epoch': 4.19}
12/08/2023 00:56:11 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-8000
 84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 8000/9555 [5:35:41<1:06:38,  2.57s/it][INFO|tokenization_utils_base.py:2210] 2023-12-08 00:56:11,433 >> tokenizer config file saved in sft_model_path/checkpoint-8000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-08 00:56:11,434 >> Special tokens file saved in sft_model_path/checkpoint-8000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-08 00:56:12,036 >> Deleting older checkpoint [sft_model_path/checkpoint-7500] due to args.save_total_limit


































































































 85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                       | 8099/9555 [5:39:53<1:00:44,  2.50s/it]



































































































 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                      | 8199/9555 [5:44:10<56:47,  2.51s/it]





































































































 87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                    | 8300/9555 [5:48:24<54:14,  2.59s/it]



































































































 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                   | 8399/9555 [5:52:35<48:32,  2.52s/it]




































































































 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 8499/9555 [5:56:49<46:41,  2.65s/it]
 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 8500/9555 [5:56:52<45:42,  2.60s/it][INFO|trainer.py:3081] 2023-12-08 01:17:21,816 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-08 01:17:21,816 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-08 01:17:21,816 >>   Batch size = 1







 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 145/155 [00:14<00:00, 10.78it/s]
{'eval_loss': 0.23761658370494843, 'eval_runtime': 17.1481, 'eval_samples_per_second': 18.019, 'eval_steps_per_second': 9.039, 'epoch': 4.45}
12/08/2023 01:17:38 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-8500
[INFO|tokenization_utils_base.py:2217] 2023-12-08 01:17:39,228 >> Special tokens file saved in sft_model_path/checkpoint-8500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-08 01:17:39,712 >> Deleting older checkpoint [sft_model_path/checkpoint-8000] due to args.save_total_limit

































































































 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 8599/9555 [6:01:20<39:50,  2.50s/it]





































































































 91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊              | 8700/9555 [6:05:40<36:07,  2.54s/it]




































































































 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌            | 8800/9555 [6:09:54<31:50,  2.53s/it]




































































































 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏          | 8900/9555 [6:14:06<27:26,  2.51s/it]



































































































 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 9000/9555 [6:18:16<23:15,  2.51s/it][INFO|trainer.py:3081] 2023-12-08 01:38:45,929 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-08 01:38:45,929 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-08 01:38:45,929 >>   Batch size = 1
{'loss': 0.247, 'learning_rate': 8.917454990655661e-08, 'epoch': 4.71}







[INFO|tokenization_utils_base.py:2217] 2023-12-08 01:39:02,728 >> Special tokens file saved in sft_model_path/checkpoint-9000/special_tokens_map.json
{'eval_loss': 0.2374292016029358, 'eval_runtime': 16.5325, 'eval_samples_per_second': 18.691, 'eval_steps_per_second': 9.375, 'epoch': 4.71}
12/08/2023 01:39:02 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-9000
12/08/2023 01:39:02 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2894] 2023-12-08 01:39:03,234 >> Deleting older checkpoint [sft_model_path/checkpoint-8500] due to args.save_total_limit



































































































 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍       | 9100/9555 [6:22:44<18:32,  2.45s/it]



































































































 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████      | 9199/9555 [6:26:52<14:32,  2.45s/it]




































































































 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 9299/9555 [6:31:05<10:34,  2.48s/it]




































































































 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 9399/9555 [6:35:16<06:39,  2.56s/it]




































































































 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 9499/9555 [6:39:24<02:17,  2.45s/it]
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 9500/9555 [6:39:26<02:16,  2.48s/it][INFO|trainer.py:3081] 2023-12-08 01:59:56,469 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-08 01:59:56,469 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-08 01:59:56,469 >>   Batch size = 1







 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                 | 138/155 [00:13<00:01, 10.89it/s]
{'eval_loss': 0.2373848259449005, 'eval_runtime': 16.6522, 'eval_samples_per_second': 18.556, 'eval_steps_per_second': 9.308, 'epoch': 4.97}
12/08/2023 02:00:13 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-9500
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 9500/9555 [6:39:43<02:16,  2.48s/it][INFO|tokenization_utils_base.py:2210] 2023-12-08 02:00:13,383 >> tokenizer config file saved in sft_model_path/checkpoint-9500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-08 02:00:13,383 >> Special tokens file saved in sft_model_path/checkpoint-9500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-08 02:00:13,885 >> Deleting older checkpoint [sft_model_path/checkpoint-9000] due to args.save_total_limit





















































100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9555/9555 [6:42:00<00:00,  2.70s/it][INFO|trainer.py:1934] 2023-12-08 02:02:30,752 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9555/9555 [6:42:01<00:00,  2.52s/it]
[INFO|tokenization_utils_base.py:2210] 2023-12-08 02:02:31,116 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-08 02:02:31,117 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-12-08 02:02:31,128 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-08 02:02:31,128 >>   Num examples = 309
[INFO|trainer.py:3086] 2023-12-08 02:02:31,129 >>   Batch size = 1
{'train_runtime': 24131.6064, 'train_samples_per_second': 6.336, 'train_steps_per_second': 0.396, 'train_loss': 0.39943191906596154, 'epoch': 5.0}
12/08/2023 02:02:30 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
12/08/2023 02:02:30 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.3994
  train_runtime            = 6:42:11.60
  train_samples_per_second =      6.336
  train_steps_per_second   =      0.396







 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 153/155 [00:14<00:00, 11.05it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_loss               =     0.2374
  eval_runtime            = 0:00:16.90
  eval_samples_per_second =     18.282
  eval_steps_per_second   =      9.171

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 155/155 [00:15<00:00, 10.13it/s]