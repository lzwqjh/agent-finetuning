
  0%|                                                                                                                                                              | 0/150 [00:00<?, ?it/s]



































































































 66%|██████████████████████████████████████████████████████████████████████████████████████████████████▎                                                  | 99/150 [03:49<01:59,  2.35s/it]
 67%|██████████████████████████████████████████████████████████████████████████████████████████████████▋                                                 | 100/150 [03:51<01:55,  2.31s/it][INFO|trainer.py:3081] 2023-10-18 14:40:55,791 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-18 14:40:55,792 >>   Num examples = 2
[INFO|trainer.py:3086] 2023-10-18 14:40:55,792 >>   Batch size = 1
  0%|                                                                                                                                                                | 0/1 [00:00<?, ?it/s]
{'eval_loss': 2.4499545097351074, 'eval_runtime': 1.2341, 'eval_samples_per_second': 1.621, 'eval_steps_per_second': 0.81, 'epoch': 19.51}
10/18/2023 14:40:57 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-100
[INFO|tokenization_utils_base.py:2217] 2023-10-18 14:40:57,274 >> Special tokens file saved in sft_model_path/checkpoint-100/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-18 14:40:57,849 >> Deleting older checkpoint [sft_model_path/checkpoint-1000] due to args.save_total_limit
















































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [05:49<00:00,  2.39s/it]
{'train_runtime': 359.5328, 'train_samples_per_second': 13.518, 'train_steps_per_second': 0.417, 'train_loss': 1.9826510938008626, 'epoch': 29.27}
10/18/2023 14:42:53 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
10/18/2023 14:42:53 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =      29.27
  train_loss               =     1.9827
  train_runtime            = 0:05:59.53
  train_samples_per_second =     13.518
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [05:49<00:00,  2.39s/it][INFO|trainer.py:1934] 2023-10-18 14:42:53,225 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [05:49<00:00,  2.33s/it]
[INFO|tokenization_utils_base.py:2210] 2023-10-18 14:42:53,566 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-10-18 14:42:53,566 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-10-18 14:42:53,575 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-18 14:42:53,575 >>   Num examples = 2
[INFO|trainer.py:3086] 2023-10-18 14:42:53,575 >>   Batch size = 1
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 693.50it/s]
***** eval metrics *****
  epoch                   =      29.27
  eval_loss               =     2.4293
  eval_runtime            = 0:00:00.92
  eval_samples_per_second =       2.17
  eval_steps_per_second   =      1.085
  perplexity              =    11.3513