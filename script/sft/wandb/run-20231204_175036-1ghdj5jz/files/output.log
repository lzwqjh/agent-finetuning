

  0%|                                                                                                                                       | 1/1827 [00:04<2:05:09,  4.11s/it]



































































































  5%|███████▎                                                                                                                             | 100/1827 [03:54<1:05:55,  2.29s/it]



































































































 11%|██████████████▋                                                                                                                        | 199/1827 [07:33<59:23,  2.19s/it]




































































































 16%|██████████████████████                                                                                                                 | 299/1827 [11:14<57:44,  2.27s/it]




































































































 22%|█████████████████████████████▍                                                                                                         | 399/1827 [14:59<52:15,  2.20s/it]




































































































 27%|████████████████████████████████████▊                                                                                                  | 499/1827 [18:44<51:08,  2.31s/it]
 27%|████████████████████████████████████▉                                                                                                  | 500/1827 [18:46<52:23,  2.37s/it][INFO|trainer.py:3081] 2023-12-04 18:09:31,412 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-04 18:09:31,412 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-04 18:09:31,412 >>   Batch size = 1

[INFO|tokenization_utils_base.py:2217] 2023-12-04 18:09:36,746 >> Special tokens file saved in sft_model_path/checkpoint-500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-04 18:09:37,208 >> Deleting older checkpoint [sft_model_path/checkpoint-3000] due to args.save_total_limit
{'eval_loss': 1.1621912717819214, 'eval_runtime': 5.0862, 'eval_samples_per_second': 19.464, 'eval_steps_per_second': 9.83, 'epoch': 0.82}
12/04/2023 18:09:36 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-500
12/04/2023 18:09:36 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



































































































 33%|████████████████████████████████████████████▎                                                                                          | 600/1827 [22:40<45:47,  2.24s/it]



































































































 38%|███████████████████████████████████████████████████▋                                                                                   | 700/1827 [26:24<45:53,  2.44s/it]



































































































 44%|███████████████████████████████████████████████████████████                                                                            | 799/1827 [30:10<40:51,  2.38s/it]




































































































 49%|██████████████████████████████████████████████████████████████████▍                                                                    | 899/1827 [33:49<33:10,  2.14s/it]




































































































 55%|█████████████████████████████████████████████████████████████████████████▊                                                             | 999/1827 [37:29<31:04,  2.25s/it]
 55%|█████████████████████████████████████████████████████████████████████████▎                                                            | 1000/1827 [37:31<30:48,  2.24s/it][INFO|trainer.py:3081] 2023-12-04 18:28:16,734 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-04 18:28:16,734 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-04 18:28:16,734 >>   Batch size = 1


 82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                        | 41/50 [00:04<00:00,  9.58it/s]
{'eval_loss': 1.1367111206054688, 'eval_runtime': 6.2541, 'eval_samples_per_second': 15.83, 'eval_steps_per_second': 7.995, 'epoch': 1.64}
12/04/2023 18:28:22 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1000
[INFO|tokenization_utils_base.py:2217] 2023-12-04 18:28:23,238 >> Special tokens file saved in sft_model_path/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-04 18:28:23,699 >> Deleting older checkpoint [sft_model_path/checkpoint-500] due to args.save_total_limit



































































































 60%|████████████████████████████████████████████████████████████████████████████████▋                                                     | 1100/1827 [41:22<27:08,  2.24s/it]



































































































 66%|███████████████████████████████████████████████████████████████████████████████████████▉                                              | 1199/1827 [44:59<22:42,  2.17s/it]





































































































 71%|███████████████████████████████████████████████████████████████████████████████████████████████▎                                      | 1300/1827 [48:39<18:53,  2.15s/it]




































































































 77%|██████████████████████████████████████████████████████████████████████████████████████████████████████▋                               | 1400/1827 [52:20<15:19,  2.15s/it]



































































































 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████                        | 1500/1827 [55:56<11:49,  2.17s/it][INFO|trainer.py:3081] 2023-12-04 18:46:41,545 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-04 18:46:41,545 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-04 18:46:41,545 >>   Batch size = 1
{'loss': 1.1726, 'learning_rate': 8.169768066681172e-07, 'epoch': 2.46}

[INFO|tokenization_utils_base.py:2217] 2023-12-04 18:46:46,911 >> Special tokens file saved in sft_model_path/checkpoint-1500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-04 18:46:47,370 >> Deleting older checkpoint [sft_model_path/checkpoint-1000] due to args.save_total_limit
{'eval_loss': 1.1292039155960083, 'eval_runtime': 5.1153, 'eval_samples_per_second': 19.354, 'eval_steps_per_second': 9.775, 'epoch': 2.46}
12/04/2023 18:46:46 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1500
12/04/2023 18:46:46 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.



































































































 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                | 1600/1827 [59:40<08:11,  2.16s/it]



































































































 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 1699/1827 [1:03:25<05:19,  2.50s/it]




































































































 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 1799/1827 [1:07:13<01:01,  2.21s/it]



























100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1827/1827 [1:08:18<00:00,  2.21s/it][INFO|trainer.py:1934] 2023-12-04 18:59:03,416 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1827/1827 [1:08:18<00:00,  2.24s/it]
[INFO|tokenization_utils_base.py:2210] 2023-12-04 18:59:03,752 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-04 18:59:03,752 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-12-04 18:59:03,762 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-04 18:59:03,762 >>   Num examples = 99
[INFO|trainer.py:3086] 2023-12-04 18:59:03,762 >>   Batch size = 1
{'train_runtime': 4108.6106, 'train_samples_per_second': 7.117, 'train_steps_per_second': 0.445, 'train_loss': 1.2600896216247925, 'epoch': 3.0}
12/04/2023 18:59:03 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
12/04/2023 18:59:03 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     1.2601
  train_runtime            = 1:08:28.61
  train_samples_per_second =      7.117
  train_steps_per_second   =      0.445

 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋                              | 39/50 [00:03<00:00, 12.09it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_loss               =     1.1286
  eval_runtime            = 0:00:05.17
  eval_samples_per_second =     19.145
  eval_steps_per_second   =      9.669

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.43it/s]