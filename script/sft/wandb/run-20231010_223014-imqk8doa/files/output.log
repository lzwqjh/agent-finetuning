
  0%|                                                                                                                                       | 0/1857 [00:00<?, ?it/s]



































































































  5%|██████▌                                                                                                                     | 99/1857 [03:41<1:02:05,  2.12s/it]
  5%|██████▌                                                                                                                    | 100/1857 [03:43<1:02:17,  2.13s/it][INFO|trainer.py:3081] 2023-10-10 22:34:06,440 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 22:34:06,440 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 22:34:06,440 >>   Batch size = 1


 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍       | 48/51 [00:03<00:00, 11.77it/s]




































































































 11%|█████████████▏                                                                                                             | 200/1857 [07:28<1:00:16,  2.18s/it][INFO|trainer.py:3081] 2023-10-10 22:37:51,326 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 22:37:51,327 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 22:37:51,327 >>   Batch size = 1
{'loss': 1.9424, 'learning_rate': 9.84952647081155e-06, 'epoch': 0.32}

 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍            | 46/51 [00:03<00:00, 12.86it/s]




































































































 16%|████████████████████▏                                                                                                        | 299/1857 [11:04<53:43,  2.07s/it]
 16%|████████████████████▏                                                                                                        | 300/1857 [11:06<53:48,  2.07s/it][INFO|trainer.py:3081] 2023-10-10 22:41:29,007 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 22:41:29,007 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 22:41:29,007 >>   Batch size = 1


 65%|██████████████████████████████████████████████████████████████████████████████████▊                                             | 33/51 [00:02<00:01, 13.16it/s]



































































































 21%|██████████████████████████▊                                                                                                  | 399/1857 [15:04<58:25,  2.40s/it]
 22%|██████████████████████████▉                                                                                                  | 400/1857 [15:06<58:48,  2.42s/it][INFO|trainer.py:3081] 2023-10-10 22:45:29,405 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 22:45:29,405 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 22:45:29,405 >>   Batch size = 1


 69%|███████████████████████████████████████████████████████████████████████████████████████▊                                        | 35/51 [00:02<00:01, 12.93it/s]



































































































 27%|█████████████████████████████████▌                                                                                           | 499/1857 [19:09<53:59,  2.39s/it]
 27%|█████████████████████████████████▋                                                                                           | 500/1857 [19:11<52:04,  2.30s/it][INFO|trainer.py:3081] 2023-10-10 22:49:34,153 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 22:49:34,154 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 22:49:34,154 >>   Batch size = 1


[INFO|tokenization_utils_base.py:2217] 2023-10-10 22:49:39,489 >> Special tokens file saved in sft_model_path/checkpoint-500/special_tokens_map.json
{'eval_loss': 0.7588748931884766, 'eval_runtime': 5.0987, 'eval_samples_per_second': 39.422, 'eval_steps_per_second': 10.003, 'epoch': 0.81}
10/10/2023 22:49:39 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-500
10/10/2023 22:49:39 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.


































































































 32%|████████████████████████████████████████▍                                                                                    | 600/1857 [22:46<42:41,  2.04s/it][INFO|trainer.py:3081] 2023-10-10 22:53:09,108 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 22:53:09,109 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 22:53:09,109 >>   Batch size = 1
 10%|████████████▋                                                                                                                    | 5/51 [00:00<00:02, 15.41it/s]


 61%|█████████████████████████████████████████████████████████████████████████████▊                                                  | 31/51 [00:02<00:01, 13.22it/s]


































































































 38%|███████████████████████████████████████████████                                                                              | 700/1857 [26:16<39:40,  2.06s/it][INFO|trainer.py:3081] 2023-10-10 22:56:39,546 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 22:56:39,546 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 22:56:39,546 >>   Batch size = 1
 10%|████████████▋                                                                                                                    | 5/51 [00:00<00:03, 15.25it/s]


 61%|█████████████████████████████████████████████████████████████████████████████▊                                                  | 31/51 [00:02<00:01, 12.16it/s]



































































































 43%|█████████████████████████████████████████████████████▊                                                                       | 799/1857 [29:44<35:57,  2.04s/it]
 43%|█████████████████████████████████████████████████████▊                                                                       | 800/1857 [29:46<35:46,  2.03s/it][INFO|trainer.py:3081] 2023-10-10 23:00:09,235 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 23:00:09,235 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 23:00:09,235 >>   Batch size = 1

 84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉                    | 43/51 [00:03<00:00, 13.04it/s]




































































































 48%|████████████████████████████████████████████████████████████▌                                                                | 900/1857 [33:18<32:32,  2.04s/it][INFO|trainer.py:3081] 2023-10-10 23:03:41,650 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 23:03:41,650 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 23:03:41,650 >>   Batch size = 1
{'loss': 0.5449, 'learning_rate': 5.518015945829337e-06, 'epoch': 1.45}

 78%|████████████████████████████████████████████████████████████████████████████████████████████████████▍                           | 40/51 [00:03<00:00, 12.46it/s]




































































































 54%|███████████████████████████████████████████████████████████████████▏                                                         | 999/1857 [36:48<29:19,  2.05s/it]
 54%|██████████████████████████████████████████████████████████████████▊                                                         | 1000/1857 [36:50<29:35,  2.07s/it][INFO|trainer.py:3081] 2023-10-10 23:07:13,687 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 23:07:13,687 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 23:07:13,687 >>   Batch size = 1

 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉               | 45/51 [00:03<00:00, 12.96it/s]
{'eval_loss': 0.607498824596405, 'eval_runtime': 5.3235, 'eval_samples_per_second': 37.757, 'eval_steps_per_second': 9.58, 'epoch': 1.61}
10/10/2023 23:07:19 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1000
[INFO|tokenization_utils_base.py:2217] 2023-10-10 23:07:19,249 >> Special tokens file saved in sft_model_path/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-10 23:07:19,693 >> Deleting older checkpoint [sft_model_path/checkpoint-500] due to args.save_total_limit

































































































 59%|█████████████████████████████████████████████████████████████████████████▍                                                  | 1099/1857 [40:18<25:52,  2.05s/it]
 59%|█████████████████████████████████████████████████████████████████████████▍                                                  | 1100/1857 [40:20<25:37,  2.03s/it][INFO|trainer.py:3081] 2023-10-10 23:10:43,594 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 23:10:43,594 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 23:10:43,594 >>   Batch size = 1


100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:04<00:00,  9.36it/s]



































































































 65%|████████████████████████████████████████████████████████████████████████████████▏                                           | 1200/1857 [43:52<22:38,  2.07s/it][INFO|trainer.py:3081] 2023-10-10 23:14:15,149 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 23:14:15,149 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 23:14:15,149 >>   Batch size = 1
{'loss': 0.5491, 'learning_rate': 2.9633956882401215e-06, 'epoch': 1.94}


 84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉                    | 43/51 [00:03<00:00, 12.57it/s]



































































































 70%|██████████████████████████████████████████████████████████████████████████████████████▊                                     | 1300/1857 [47:22<18:51,  2.03s/it][INFO|trainer.py:3081] 2023-10-10 23:17:45,591 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 23:17:45,591 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 23:17:45,591 >>   Batch size = 1
{'loss': 0.5133, 'learning_rate': 2.2017863431892534e-06, 'epoch': 2.1}


 84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉                    | 43/51 [00:03<00:00, 12.87it/s]



































































































 75%|█████████████████████████████████████████████████████████████████████████████████████████████▍                              | 1399/1857 [50:50<15:50,  2.08s/it]
 75%|█████████████████████████████████████████████████████████████████████████████████████████████▍                              | 1400/1857 [50:52<15:41,  2.06s/it][INFO|trainer.py:3081] 2023-10-10 23:21:14,867 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 23:21:14,867 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 23:21:14,867 >>   Batch size = 1


 65%|██████████████████████████████████████████████████████████████████████████████████▊                                             | 33/51 [00:02<00:01, 11.94it/s]


































































































 81%|████████████████████████████████████████████████████████████████████████████████████████████████████                        | 1499/1857 [54:21<12:04,  2.02s/it]
 81%|████████████████████████████████████████████████████████████████████████████████████████████████████▏                       | 1500/1857 [54:23<12:04,  2.03s/it][INFO|trainer.py:3081] 2023-10-10 23:24:46,293 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 23:24:46,293 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 23:24:46,293 >>   Batch size = 1


 82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍                      | 42/51 [00:04<00:00, 10.10it/s]
{'eval_loss': 0.5990278124809265, 'eval_runtime': 6.459, 'eval_samples_per_second': 31.119, 'eval_steps_per_second': 7.896, 'epoch': 2.42}
10/10/2023 23:24:52 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1500
[INFO|tokenization_utils_base.py:2217] 2023-10-10 23:24:52,994 >> Special tokens file saved in sft_model_path/checkpoint-1500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-10 23:24:53,452 >> Deleting older checkpoint [sft_model_path/checkpoint-1000] due to args.save_total_limit


































































































 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 1599/1857 [58:07<08:50,  2.05s/it]
 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 1600/1857 [58:09<08:44,  2.04s/it][INFO|trainer.py:3081] 2023-10-10 23:28:32,634 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 23:28:32,634 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 23:28:32,634 >>   Batch size = 1


 61%|█████████████████████████████████████████████████████████████████████████████▊                                                  | 31/51 [00:02<00:01, 12.81it/s]



































































































 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 1700/1857 [1:01:49<06:07,  2.34s/it][INFO|trainer.py:3081] 2023-10-10 23:32:12,422 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 23:32:12,423 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 23:32:12,423 >>   Batch size = 1
{'loss': 0.513, 'learning_rate': 1.934778843863766e-07, 'epoch': 2.74}


 69%|███████████████████████████████████████████████████████████████████████████████████████▊                                        | 35/51 [00:03<00:01, 11.77it/s]



































































































 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 1800/1857 [1:05:24<01:57,  2.05s/it][INFO|trainer.py:3081] 2023-10-10 23:35:46,866 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 23:35:46,866 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 23:35:46,867 >>   Batch size = 1
{'loss': 0.5023, 'learning_rate': 2.736014235001194e-08, 'epoch': 2.91}


 73%|████████████████████████████████████████████████████████████████████████████████████████████▊                                   | 37/51 [00:03<00:01, 11.89it/s]
























































100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 1856/1857 [1:07:25<00:02,  2.04s/it]
{'train_runtime': 5609.6445, 'train_samples_per_second': 10.6, 'train_steps_per_second': 0.331, 'train_loss': 1.1871192800658457, 'epoch': 3.0}
10/10/2023 23:37:50 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1857/1857 [1:07:27<00:00,  2.04s/it][INFO|trainer.py:1934] 2023-10-10 23:37:50,289 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1857/1857 [1:07:27<00:00,  2.18s/it]
[INFO|tokenization_utils_base.py:2210] 2023-10-10 23:37:50,617 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-10-10 23:37:50,618 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-10-10 23:37:50,627 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-10 23:37:50,627 >>   Num examples = 201
[INFO|trainer.py:3086] 2023-10-10 23:37:50,627 >>   Batch size = 1
  6%|███████▌                                                                                                                         | 3/51 [00:00<00:02, 19.16it/s]
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     1.1871
  train_runtime            = 1:33:29.64
  train_samples_per_second =       10.6


100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:04<00:00, 12.15it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_loss               =     0.5867
  eval_runtime            = 0:00:05.15
  eval_samples_per_second =     39.014
  eval_steps_per_second   =      9.899
  perplexity              =     1.7981