

  0%|▏                                                                                                                                                                | 1/1160 [00:06<2:01:51,  6.31s/it]


































































































  9%|█████████████▊                                                                                                                                                    | 99/1160 [05:06<54:12,  3.07s/it]





































































































 17%|███████████████████████████▊                                                                                                                                     | 200/1160 [10:15<49:43,  3.11s/it]




































































































 26%|█████████████████████████████████████████▋                                                                                                                       | 300/1160 [15:19<42:45,  2.98s/it]




































































































 34%|███████████████████████████████████████████████████████▌                                                                                                         | 400/1160 [20:22<38:43,  3.06s/it]



































































































 43%|█████████████████████████████████████████████████████████████████████▍                                                                                           | 500/1160 [25:18<32:43,  2.98s/it][INFO|trainer.py:3081] 2023-12-08 19:28:00,800 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-08 19:28:00,800 >>   Num examples = 38
[INFO|trainer.py:3086] 2023-12-08 19:28:00,801 >>   Batch size = 1
{'loss': 0.4893, 'learning_rate': 6.344599103076329e-06, 'epoch': 2.15}

 42%|█████████████████████████████████████████████████████████████████████▍                                                                                               | 8/19 [00:00<00:01,  9.93it/s]
{'eval_loss': 0.4644063711166382, 'eval_runtime': 4.1413, 'eval_samples_per_second': 9.176, 'eval_steps_per_second': 4.588, 'epoch': 2.15}
12/08/2023 19:28:04 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-500
 43%|█████████████████████████████████████████████████████████████████████▍                                                                                           | 500/1160 [25:23<32:43,  2.98s/it][INFO|tokenization_utils_base.py:2210] 2023-12-08 19:28:05,340 >> tokenizer config file saved in sft_model_path/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-08 19:28:05,341 >> Special tokens file saved in sft_model_path/checkpoint-500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-08 19:28:06,129 >> Deleting older checkpoint [sft_model_path/checkpoint-1000] due to args.save_total_limit



































































































 52%|███████████████████████████████████████████████████████████████████████████████████▎                                                                             | 600/1160 [30:12<25:59,  2.79s/it]




































































































 60%|████████████████████████████████████████████████████████████████████████████████████▍                                                       | 700/1160 [35:04<23:37,  3.08s/it]



































































































 69%|████████████████████████████████████████████████████████████████████████████████████████████████▍                                           | 799/1160 [39:52<19:26,  3.23s/it]





































































































 78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                               | 900/1160 [44:34<12:57,  2.99s/it]



































































































 86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                   | 999/1160 [49:20<07:41,  2.87s/it]
 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                   | 1000/1160 [49:23<07:32,  2.83s/it][INFO|trainer.py:3081] 2023-12-08 19:52:04,974 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-08 19:52:04,975 >>   Num examples = 38
[INFO|trainer.py:3086] 2023-12-08 19:52:04,975 >>   Batch size = 1
 74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                     | 14/19 [00:01<00:00,  8.62it/s]
{'eval_loss': 0.4223434627056122, 'eval_runtime': 4.2911, 'eval_samples_per_second': 8.856, 'eval_steps_per_second': 4.428, 'epoch': 4.3}
12/08/2023 19:52:09 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1000
[INFO|tokenization_utils_base.py:2217] 2023-12-08 19:52:09,648 >> Special tokens file saved in sft_model_path/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-12-08 19:52:10,402 >> Deleting older checkpoint [sft_model_path/checkpoint-500] due to args.save_total_limit


































































































 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋       | 1099/1160 [54:06<02:57,  2.91s/it]




























































100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1160/1160 [57:04<00:00,  2.92s/it][INFO|trainer.py:1934] 2023-12-08 19:59:46,887 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1160/1160 [57:05<00:00,  2.95s/it]
{'train_runtime': 3436.5259, 'train_samples_per_second': 5.411, 'train_steps_per_second': 0.338, 'train_loss': 0.8118383052020237, 'epoch': 4.99}
12/08/2023 19:59:46 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
12/08/2023 19:59:46 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =       4.99
  train_loss               =     0.8118
  train_runtime            = 0:57:16.52
  train_samples_per_second =      5.411
  train_steps_per_second   =      0.338
[INFO|tokenization_utils_base.py:2210] 2023-12-08 19:59:47,510 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-12-08 19:59:47,511 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-12-08 19:59:47,524 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-12-08 19:59:47,524 >>   Num examples = 38
[INFO|trainer.py:3086] 2023-12-08 19:59:47,524 >>   Batch size = 1

 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍       | 18/19 [00:01<00:00,  9.38it/s]
***** eval metrics *****
  epoch                   =       4.99
  eval_loss               =     0.4213
  eval_runtime            = 0:00:04.16
  eval_samples_per_second =      9.131
  eval_steps_per_second   =      4.565

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:02<00:00,  8.19it/s]