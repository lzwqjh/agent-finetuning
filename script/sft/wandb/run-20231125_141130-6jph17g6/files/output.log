

  0%|                                                                                                                                                                        | 1/1710 [00:03<1:47:57,  3.79s/it]



































































































  6%|█████████▊                                                                                                                                                              | 100/1710 [03:40<58:23,  2.18s/it]


































































































 12%|███████████████████▌                                                                                                                                                    | 199/1710 [07:13<55:14,  2.19s/it]



































































































 18%|█████████████████████████████▍                                                                                                                                          | 300/1710 [10:38<45:21,  1.93s/it]
































































































 23%|███████████████████████████████████████▎                                                                                                                                | 400/1710 [13:53<41:34,  1.90s/it]
































































































 29%|█████████████████████████████████████████████████                                                                                                                       | 499/1710 [17:06<39:18,  1.95s/it]
 29%|█████████████████████████████████████████████████                                                                                                                       | 500/1710 [17:08<39:33,  1.96s/it][INFO|trainer.py:3081] 2023-11-25 14:28:47,226 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-11-25 14:28:47,226 >>   Num examples = 19
[INFO|trainer.py:3086] 2023-11-25 14:28:47,226 >>   Batch size = 1
 29%|█████████████████████████████████████████████████                                                                                                                       | 500/1710 [17:09<39:33,  1.96s/it][INFO|tokenization_utils_base.py:2210] 2023-11-25 14:28:48,637 >> tokenizer config file saved in sft_model_path/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-11-25 14:28:48,638 >> Special tokens file saved in sft_model_path/checkpoint-500/special_tokens_map.json
{'eval_loss': 0.4974696636199951, 'eval_runtime': 1.1693, 'eval_samples_per_second': 16.249, 'eval_steps_per_second': 4.276, 'epoch': 8.71}
11/25/2023 14:28:48 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-500
11/25/2023 14:28:48 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
































































































 35%|██████████████████████████████████████████████████████████▊                                                                                                             | 599/1710 [20:23<36:28,  1.97s/it]



































































































 41%|████████████████████████████████████████████████████████████████████▊                                                                                                   | 700/1710 [23:55<32:38,  1.94s/it]






























































































 47%|██████████████████████████████████████████████████████████████████████████████▌                                                                                         | 800/1710 [27:11<32:03,  2.11s/it]


































































































 53%|████████████████████████████████████████████████████████████████████████████████████████▎                                                                               | 899/1710 [30:36<29:38,  2.19s/it]

































































































 58%|█████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                     | 1000/1710 [34:05<22:31,  1.90s/it][INFO|trainer.py:3081] 2023-11-25 14:45:43,765 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-11-25 14:45:43,765 >>   Num examples = 19
[INFO|trainer.py:3086] 2023-11-25 14:45:43,765 >>   Batch size = 1
{'loss': 0.4452, 'learning_rate': 3.882117978314298e-06, 'epoch': 17.43}
{'eval_loss': 0.4156137704849243, 'eval_runtime': 1.1391, 'eval_samples_per_second': 16.68, 'eval_steps_per_second': 4.389, 'epoch': 17.43}
11/25/2023 14:45:44 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1000
 58%|█████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                     | 1000/1710 [34:06<22:31,  1.90s/it][INFO|tokenization_utils_base.py:2210] 2023-11-25 14:45:45,138 >> tokenizer config file saved in sft_model_path/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-11-25 14:45:45,138 >> Special tokens file saved in sft_model_path/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:2894] 2023-11-25 14:45:45,579 >> Deleting older checkpoint [sft_model_path/checkpoint-500] due to args.save_total_limit
11/25/2023 14:45:44 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.






























































































 64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                           | 1100/1710 [37:19<19:35,  1.93s/it]
































































































 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                 | 1200/1710 [40:30<16:08,  1.90s/it]































































































 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                        | 1300/1710 [43:52<14:52,  2.18s/it]



































































































 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                              | 1399/1710 [47:27<11:27,  2.21s/it]




































































































 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                    | 1499/1710 [51:06<07:55,  2.25s/it]
 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                    | 1500/1710 [51:08<07:48,  2.23s/it][INFO|trainer.py:3081] 2023-11-25 15:02:46,858 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-11-25 15:02:46,858 >>   Num examples = 19
[INFO|trainer.py:3086] 2023-11-25 15:02:46,858 >>   Batch size = 1
 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                    | 1500/1710 [51:09<07:48,  2.23s/it][INFO|tokenization_utils_base.py:2210] 2023-11-25 15:02:48,271 >> tokenizer config file saved in sft_model_path/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-11-25 15:02:48,272 >> Special tokens file saved in sft_model_path/checkpoint-1500/special_tokens_map.json
{'eval_loss': 0.39845678210258484, 'eval_runtime': 1.1791, 'eval_samples_per_second': 16.113, 'eval_steps_per_second': 4.24, 'epoch': 26.14}
11/25/2023 15:02:48 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-1500
11/25/2023 15:02:48 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2894] 2023-11-25 15:02:48,707 >> Deleting older checkpoint [sft_model_path/checkpoint-1000] due to args.save_total_limit



































































































 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎          | 1600/1710 [54:46<03:57,  2.16s/it]


































































































 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 1700/1710 [58:14<00:18,  1.90s/it]









100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1710/1710 [58:33<00:00,  1.89s/it][INFO|trainer.py:1934] 2023-11-25 15:10:12,473 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1710/1710 [58:33<00:00,  2.05s/it]
[INFO|tokenization_utils_base.py:2210] 2023-11-25 15:10:12,755 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-11-25 15:10:12,756 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-11-25 15:10:12,765 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-11-25 15:10:12,765 >>   Num examples = 19
[INFO|trainer.py:3086] 2023-11-25 15:10:12,765 >>   Batch size = 1
 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                     | 3/5 [00:00<00:00, 19.68it/s]
{'train_runtime': 3524.2562, 'train_samples_per_second': 15.603, 'train_steps_per_second': 0.485, 'train_loss': 0.6398216431601006, 'epoch': 29.8}
11/25/2023 15:10:12 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
11/25/2023 15:10:12 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =       29.8
  train_loss               =     0.6398
  train_runtime            = 0:58:44.25
  train_samples_per_second =     15.603
  train_steps_per_second   =      0.485
***** eval metrics *****
  epoch                   =       29.8
  eval_loss               =      0.398
  eval_runtime            = 0:00:01.00
  eval_samples_per_second =     18.947
  eval_steps_per_second   =      4.986

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.88it/s]