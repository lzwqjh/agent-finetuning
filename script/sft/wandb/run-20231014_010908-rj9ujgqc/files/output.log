
  0%|                                                                                                                                        | 0/590 [00:00<?, ?it/s]



































































































 17%|█████████████████████▎                                                                                                        | 100/590 [03:34<17:08,  2.10s/it][INFO|trainer.py:3081] 2023-10-14 01:12:51,761 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-14 01:12:51,761 >>   Num examples = 20
[INFO|trainer.py:3086] 2023-10-14 01:12:51,761 >>   Batch size = 1
{'loss': 8.0953, 'learning_rate': 9.501434483772371e-06, 'epoch': 1.68}
 17%|█████████████████████▎                                                                                                        | 100/590 [03:36<17:08,  2.10s/it]



































































































 34%|██████████████████████████████████████████▍                                                                                   | 199/590 [07:11<14:47,  2.27s/it]
 34%|██████████████████████████████████████████▋                                                                                   | 200/590 [07:13<14:57,  2.30s/it][INFO|trainer.py:3081] 2023-10-14 01:16:31,045 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-14 01:16:31,045 >>   Num examples = 20
[INFO|trainer.py:3086] 2023-10-14 01:16:31,046 >>   Batch size = 1
 40%|████████████████████████████████████████████████████                                                                              | 2/5 [00:00<00:00, 16.60it/s]




































































































 51%|████████████████████████████████████████████████████████████████                                                              | 300/590 [10:47<10:57,  2.27s/it][INFO|trainer.py:3081] 2023-10-14 01:20:04,840 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-14 01:20:04,840 >>   Num examples = 20
[INFO|trainer.py:3086] 2023-10-14 01:20:04,840 >>   Batch size = 1
{'loss': 0.8866, 'learning_rate': 5.192182967278479e-06, 'epoch': 5.04}
 51%|████████████████████████████████████████████████████████████████                                                              | 300/590 [10:49<10:57,  2.27s/it]


































































































 68%|█████████████████████████████████████████████████████████████████████████████████████▍                                        | 400/590 [14:20<06:43,  2.12s/it][INFO|trainer.py:3081] 2023-10-14 01:23:37,338 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-14 01:23:37,338 >>   Num examples = 20
[INFO|trainer.py:3086] 2023-10-14 01:23:37,338 >>   Batch size = 1
{'loss': 0.854, 'learning_rate': 2.5556958833202406e-06, 'epoch': 6.72}
 68%|█████████████████████████████████████████████████████████████████████████████████████▍                                        | 400/590 [14:21<06:43,  2.12s/it]



































































































 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▊                   | 500/590 [17:55<03:09,  2.11s/it][INFO|trainer.py:3081] 2023-10-14 01:27:13,032 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-14 01:27:13,032 >>   Num examples = 20
[INFO|trainer.py:3086] 2023-10-14 01:27:13,032 >>   Batch size = 1
 60%|██████████████████████████████████████████████████████████████████████████████                                                    | 3/5 [00:00<00:00, 17.09it/s]
[INFO|tokenization_utils_base.py:2217] 2023-10-14 01:27:14,650 >> Special tokens file saved in sft_model_path/checkpoint-500/special_tokens_map.json
[INFO|trainer.py:2894] 2023-10-14 01:27:15,094 >> Deleting older checkpoint [sft_model_path/checkpoint-6000] due to args.save_total_limit
{'eval_loss': 0.8162728548049927, 'eval_runtime': 1.3812, 'eval_samples_per_second': 14.48, 'eval_steps_per_second': 3.62, 'epoch': 8.4}
10/14/2023 01:27:14 - INFO - utils.trainer - Saving model checkpoint to sft_model_path/checkpoint-500
10/14/2023 01:27:14 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
























































































100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 589/590 [21:07<00:02,  2.11s/it]
{'train_runtime': 1279.5903, 'train_samples_per_second': 14.872, 'train_steps_per_second': 0.461, 'train_loss': 2.183041672787424, 'epoch': 9.92}
10/14/2023 01:30:26 - INFO - utils.trainer - Saving model checkpoint to sft_model_path
10/14/2023 01:30:26 - INFO - utils.trainer - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
***** train metrics *****
  epoch                    =       9.92
  train_loss               =      2.183
  train_runtime            = 0:21:19.59
  train_samples_per_second =     14.872
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 590/590 [21:09<00:00,  2.10s/it][INFO|trainer.py:1934] 2023-10-14 01:30:26,368 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 590/590 [21:09<00:00,  2.15s/it]
***** eval metrics *****
  epoch                   =       9.92
  eval_loss               =     0.8168
  eval_runtime            = 0:00:01.30
  eval_samples_per_second =      15.35
  eval_steps_per_second   =      3.838
  perplexity              =     2.2632
[INFO|tokenization_utils_base.py:2210] 2023-10-14 01:30:26,751 >> tokenizer config file saved in sft_model_path/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-10-14 01:30:26,751 >> Special tokens file saved in sft_model_path/special_tokens_map.json
[INFO|trainer.py:3081] 2023-10-14 01:30:26,760 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-10-14 01:30:26,761 >>   Num examples = 20
[INFO|trainer.py:3086] 2023-10-14 01:30:26,761 >>   Batch size = 1
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.02it/s]